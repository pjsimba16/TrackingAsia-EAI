{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18030773-1148-468d-83e3-b23367309ed0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab283cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies. GUI window is created to inform the user that dependencies are being imported and will close once the process is complete.\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "sg.theme('DarkBlue13')\n",
    "\n",
    "layout = [\n",
    "    [sg.Text('Importing dependencies. Please wait.', font=('Calibri Bold', 12))]\n",
    "]\n",
    "\n",
    "window = sg.Window('Economic Activity Index: Import Dependencies',layout, use_default_focus=False, font=('Calibri',12))\n",
    "\n",
    "\n",
    "event, values = window.read(timeout=10)\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import time\n",
    "import xlwings as xw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import statistics\n",
    "import cloudscraper\n",
    "import PIL\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import io\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle, Wedge, Rectangle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from pypdf import PdfMerger\n",
    "import warnings\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "# !pip install openpyxl\n",
    "# !pip install kaleido\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import subprocess\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.styles import NamedStyle\n",
    "from openpyxl.formula.translate import Translator\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime, timezone\n",
    "import shutil\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import quantecon\n",
    "import statistics\n",
    "from cif import cif\n",
    "from functools import reduce  \n",
    "import os, sys\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import shap\n",
    "#shap.initjs()\n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bcffb-9886-4e66-be46-926e90c79500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Graphical User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8c37d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084cf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize an image from the web based on the resize parameter.\n",
    "def resize_img(url, resize):\n",
    "    data_url = (cloudscraper.create_scraper(browser={\"browser\": \"firefox\", \"platform\": \"windows\", \"mobile\": False})\n",
    "        .get(url)\n",
    "        .content)\n",
    "    img = PIL.Image.open(io.BytesIO(data_url))\n",
    "    cur_width, cur_height = img.size\n",
    "    new_width, new_height = resize\n",
    "    scale = min(new_height / cur_height, new_width / cur_width)\n",
    "    img = img.resize((int(cur_width * scale), int(cur_height * scale)), PIL.Image.Resampling.LANCZOS)\n",
    "    png_bio = io.BytesIO()\n",
    "    img.save(png_bio, format=\"PNG\")\n",
    "    png_data = png_bio.getvalue()\n",
    "    return png_data\n",
    "\n",
    "#Open the home page screen of the GUI.\n",
    "def UI_home_page_design():\n",
    "    png_data_adb = resize_img('https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Asian_Development_Bank_logo.svg/2048px-Asian_Development_Bank_logo.svg.png', (118,118))\n",
    "    png_data_banner = resize_img('https://asianbondsonline.adb.org/macroeconomictracker/images/woman-planting-with-charts-trackingasia2.jpg', (300, 300))\n",
    "\n",
    "    intro_txt = \"Developed by the Asian Development Bank for the TrackingAsia project, the Economic Activity Index enables monthly tracking of business cycles by utilizing economic series from six categories â€”consumption, investment, trade, government, financial, and the external sector. Further, this tool identifies the key drivers behind economic expansions and downturns over time. The goal of this tool is to provide invaluable insights into economic trends, facilitating informed decision-making on a global scale. This tool is designed to empowers users to utilize their unique datasets in constructing their own Economic Activity Index.\"\n",
    "    welcome_txt = sg.Text('Economic Activity Index Modelling Interface', font=('Calibri Bold', 14, \"bold\"), expand_x=True, justification='center')\n",
    "    intro_txt = sg.Text(intro_txt, size=(40, None), font=('Calibri', 12), expand_x=True, justification='center')\n",
    "    continue_button = sg.Button('Continue', key='CONTINUE_HOME', font=('Calibri', 8), expand_x=True)\n",
    "    exit = sg.Button('Exit', key='EXIT', font=('Calibri', 8), button_color='red', expand_x=True)\n",
    "    adb_banner = sg.Image(data=png_data_adb, key='-ADB-', pad=(0,0))\n",
    "    banner = sg.Image(data=png_data_banner, key='-IMAGE-', pad=(0,0))\n",
    "    layout = [\n",
    "        [adb_banner, banner],\n",
    "        [welcome_txt],\n",
    "        [intro_txt],\n",
    "        [continue_button, exit]\n",
    "    ]\n",
    "    window = sg.Window('Economic Activity Index: Home Page', layout, grab_anywhere=True, resizable=True, font=('Calibri',12))\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'CONTINUE_HOME':\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED or event == 'EXIT':\n",
    "            break\n",
    "    window.close()\n",
    "    return event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deb56e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Excel File Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee8016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the screen to browse the local device and select the excel file to be used in the analysis.\n",
    "#Also checks if the file is formatted correctly\n",
    "def UI_add_excel_data(): \n",
    "    layout = [\n",
    "                  [sg.Button(\"Back\", key='BACK_HOME', expand_x=False, font=('Calibri',8))],\n",
    "                  [sg.Text(f'Select the Excel file template containing features that will be used to forecast the Economic Activity Index',\n",
    "                          justification='center', expand_x=True, font=('Calibri', 12))],\n",
    "                  [sg.Text('Excel File', font=('Calibri',12)), sg.Input(key='Input_1', size=(100,1)), \n",
    "                   sg.FileBrowse(font=('Calibri', 8), tooltip='Click to locate the Excel File template.')],\n",
    "                  [sg.Button(\"Submit\", font=('Calibri', 8), enable_events=True, key='SUBMIT', expand_x=False),\n",
    "                   sg.Button(\"Exit\", key='EXIT', button_color='red', expand_x=False, font=('Calibri', 8))]\n",
    "              ]\n",
    "\n",
    "    window = sg.Window('Economic Activity Index: Add Excel Data',layout, resizable=True, use_default_focus=False, font=('Calibri',12))\n",
    "    filename_template = 'EAI_excel_template'\n",
    "    \n",
    "    all_data = {}\n",
    "    fpath = ''\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == sg.WINDOW_CLOSED or event == 'EXIT':\n",
    "            break\n",
    "        if event == 'BACK_HOME':\n",
    "            break\n",
    "        if event == 'SUBMIT':\n",
    "            if filename_template in values['Input_1']: #check if correct filename\n",
    "                fpath = values['Input_1']\n",
    "                df_qtr_info = pd.read_excel(values['Input_1'], sheet_name=1)\n",
    "                df_qtr_data = pd.read_excel(values['Input_1'], sheet_name=2)\n",
    "                df_month_info = pd.read_excel(values['Input_1'], sheet_name=3)\n",
    "                df_month_data = pd.read_excel(values['Input_1'], sheet_name=4)\n",
    "                df_info = pd.concat([df_qtr_info, df_month_info], axis=0)\n",
    "                for i in df_info.values:\n",
    "                    if i[1] == 'Q':\n",
    "                        all_data[i[0]] = {'name': i[0],\n",
    "                                         'category': i[6],\n",
    "                                         'series_name': i[4],\n",
    "                                         'frequency': i[1],\n",
    "                                         'include': i[7],\n",
    "                                         'diff': i[8],\n",
    "                                         'year': i[9],\n",
    "                                         'norm': i[10],\n",
    "                                         'edited': 0, \n",
    "                                         'Data': df_qtr_data[['date', i[0]]]}\n",
    "                    elif i[1] == 'M':\n",
    "                        all_data[i[0]] = {'name': i[0],\n",
    "                                         'category': i[6],\n",
    "                                         'series_name': i[4],\n",
    "                                         'frequency': i[1],\n",
    "                                         'include': i[7],\n",
    "                                         'diff': i[8],\n",
    "                                         'year': i[9],\n",
    "                                         'norm': i[10],\n",
    "                                         'edited': 0, \n",
    "                                         'Data': df_month_data[['date', i[0]]]}\n",
    "                event = 'SUBMIT_EXCEL'\n",
    "                \n",
    "                if 'target_variable' not in df_info.Category.values:\n",
    "                    event = 'BACK_CHECK_EXCEL'\n",
    "                    sg.popup(\"No target variable found in the Excel File, please make sure to include one target variable.\",\n",
    "                             font=('Calibri', 12), button_justification='center', title='Error Warning!')\n",
    "                    \n",
    "                if (df_info.Category == 'target_variable').sum() > 1:\n",
    "                    event = 'BACK_CHECK_EXCEL'\n",
    "                    sg.popup(\"More than 1 target variable found in the Excel File, please make sure to only include one target variable.\",\n",
    "                             font=('Calibri', 12), button_justification='center', title= 'Error Warning!')\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                sg.popup('Please select the correct file template. Return to the previous screen and upload the correct file.',\n",
    "                        font=('Calibri', 12), button_justification='center', title='Error Warning!')\n",
    "    window.close()\n",
    "    return event, all_data, fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9f8a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Indicator Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7ce70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open UI screen that shows all indicators in the chosen file within their respective economic buckets\n",
    "def UI_choose_indicators(all_data={}): \n",
    "\n",
    "    back_button = sg.Button('Back', key='BACK_CHECK_EXCEL', font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Select indicators to use for the model(s)', font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    instructions_txt = sg.Text(\"Select the indicators that you'd like to be incorporated into the model(s).\", font=('Calibri', 12), expand_x=True, justification='center')\n",
    "    \n",
    "    country_contributions = {'country': [],\n",
    "                                 'target_variable': [],\n",
    "                                 'consumption': [],\n",
    "                                 'exo': [],\n",
    "                                 'financial': [],\n",
    "                                 'government': [],\n",
    "                                 'investment': [],\n",
    "                                 'trade': []}\n",
    "\n",
    "    inspect_data_button = sg.Button('   Inspect Data    ', key='INSPECT_INDICATORS', font=('Calibri', 8), expand_x=True, tooltip='Click to view visualizations and summary statistics on the indicator(s).')\n",
    "    reconfigure_data_button = sg.Button(' Edit Processing ', key='RECONFIGURE_INDICATORS', font=('Calibri', 8), expand_x=True, tooltip='Click to view / edit / overwrite the inputs in the Excel File.')\n",
    "    \n",
    "    ok_button = sg.Button('Ok', key='OK_CHOOSE_INDICATORS', font=('Calibri', 8), expand_x=True, auto_size_button=False)\n",
    "    exit = sg.Button('  Exit  ', key='EXIT', font=('Calibri', 8), button_color='red', expand_x=True, auto_size_button=False)\n",
    "\n",
    "    layout = [\n",
    "        [back_button],\n",
    "        [title_txt],\n",
    "        [instructions_txt],\n",
    "    ]\n",
    "\n",
    "    column_layout = []\n",
    "    if all_data != {}:\n",
    "        for key in all_data.keys():\n",
    "            country_contributions[all_data[key]['category']].append(all_data[key]['name'])\n",
    "\n",
    "\n",
    "    for key in country_contributions.keys():\n",
    "        new_ls = country_contributions[key]\n",
    "        new_ls_2 = []\n",
    "        for l in new_ls:\n",
    "            if all_data[l]['include'] == 1:\n",
    "                new_ls_2.append(l)\n",
    "        #for i in range(len(new_ls)):\n",
    "         #   if all_data[new_ls[i]]['include'] == 0:\n",
    "          #      new_ls[i] = sg.Checkbox(textwrap.fill(new_ls[i],25), default=False, key = 'contributions-->'+key+'-->'+new_ls[i], font=('Calibri', 6))\n",
    "           # else:\n",
    "            #    new_ls[i] = sg.Checkbox(textwrap.fill(new_ls[i],25), default=True, key = 'contributions-->'+key+'-->'+new_ls[i], font=('Calibri', 6))\n",
    "        if key != 'country':\n",
    "            column_layout.append([sg.Text(key.capitalize()+' Indicators:', key='-OUT-', font=('Calibri Bold', 8), expand_x=True, justification='left')])\n",
    "            #column_layout.append([sg.Column([new_ls])])\n",
    "            column_layout.append([sg.Listbox(new_ls_2, key='LISTBOX', font=('Calibri', 8), expand_x=True,pad=(0,0),expand_y=True,background_color=sg.theme_background_color(),text_color='white')])\n",
    "\n",
    "    layout.append([[sg.Column(column_layout, scrollable=True, expand_x=True)],[sg.Push(),inspect_data_button, reconfigure_data_button, sg.Push()],[ok_button,exit]])\n",
    "\n",
    "    window = sg.Window('Economic Activity Index: Indicator Selection', layout, grab_anywhere=True,resizable=True, font=('Calibri',12))\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'OK_CHOOSE_INDICATORS':\n",
    "            break\n",
    "        if event == 'INSPECT_INDICATORS':\n",
    "            break\n",
    "        if event == 'RECONFIGURE_INDICATORS':\n",
    "            break\n",
    "        elif event == 'BACK_CHECK_EXCEL':\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED or event == 'EXIT':\n",
    "            break\n",
    "    \n",
    "    window.close()\n",
    "    return event, values, all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591bdbea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dacf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_UP =    'â–²'\n",
    "SYMBOL_DOWN =  'â–¼'\n",
    "\n",
    "model_list = ['Linear Regression','Random Forest','Light Gradient Boosting Method (LGBM)','Extreme Boosting (XGBoost)','CatBoost','Stochastic Gradient Descent (SGDR)','Kernel Ridge','Elastic Net','Bayesian Ridge','Gradient Boosting','Support Vector Machine (SVM)']\n",
    "\n",
    "models_explained_text = \"\"\"Linear regression: \n",
    "Plots a line of best fit for each type of linear regression using the least squares method.\n",
    "\n",
    "Random Forest: \n",
    "Uses ensemble learning with multiple decision trees to find more accurate predictions.\n",
    "\n",
    "Light Gradient Boosting Method (LGBM): \n",
    "More efficient and lightweight gradient boosting method.\n",
    "\n",
    "Extreme Boosting (XGBoost): \n",
    "Uses advanced regularization to reduce overfitting. Usually performs better with larger datasets.\n",
    "\n",
    "CatBoost: \n",
    "Builds upon the theory of decision trees and gradient boosting by growing oblivious trees to simplify the model and improve efficiency. \n",
    "\n",
    "Stochastic Gradient Descent (SGDR): \n",
    "Iterative method that optimizes an objective function and reduces the computational burden of bigger datasets.\n",
    "\n",
    "Kernel Ridge: \n",
    "Uses a kernel function to calculate weights. This model is used when there is too much data for a traditional linear model.\n",
    "\n",
    "Elastic Net: \n",
    "Uses lasso and ridge techniques to improve the regularization of models.\n",
    "\n",
    "Bayesian Ridge: \n",
    "Uses linear regression with probability distributors instead of point estimates.\n",
    "\n",
    "Gradient Boosting: \n",
    "An ensemble of weak learners/models are used to create a single, more accurate model.\n",
    "\n",
    "Support Vector Machine (SVM): \n",
    "Finds the hyperplane that passes through as many data points as possible within a certain distance to minimize prediction error.\n",
    " \"\"\"\n",
    "\n",
    "#Creates a collapsible column element within a UI screen\n",
    "def Collapsible(layout, key, title='', arrows=(sg.SYMBOL_DOWN, sg.SYMBOL_UP), collapsed=False):\n",
    "    \"\"\"\n",
    "    User Defined Element\n",
    "    A \"collapsable section\" element. Like a container element that can be collapsed and brought back\n",
    "    :param layout:Tuple[List[sg.Element]]: The layout for the section\n",
    "    :param key:Any: Key used to make this section visible / invisible\n",
    "    :param title:str: Title to show next to arrow\n",
    "    :param arrows:Tuple[str, str]: The strings to use to show the section is (Open, Closed).\n",
    "    :param collapsed:bool: If True, then the section begins in a collapsed state\n",
    "    :return:sg.Column: Column including the arrows, title and the layout that is pinned\n",
    "    \"\"\"\n",
    "    return sg.Column([[sg.pin(sg.Column(layout, key=key, visible=not collapsed, metadata=arrows,scrollable=True))]], pad=(0,0))\n",
    "\n",
    "#Opens the model selection screen in the GUI\n",
    "def UI_select_model():\n",
    "    \n",
    "    back_button = sg.Button('Back', key='BACK_SELECT_INDICATORS', font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Select the model(s) to be used:', font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    instructions_txt = sg.Text('Select the machine learning model(s) to run and the corresponding start date for training the model(s).\\nBy default, 50% of the dataset will be initialized as the training dataset unless otherwise stated below.', \n",
    "                               font=('Calibri', 12), expand_x=True, justification='center')\n",
    "    model_explanations = sg.Text(models_explained_text, key='-OUT-', font=('Calibri', 8), expand_x=True, justification='left')\n",
    "\n",
    "    checkbox_ls = []\n",
    "    for m in model_list:\n",
    "        checkbox_ls.append([sg.Checkbox(m, key = m, font=('Calibri', 8), expand_x = True,default=False)])\n",
    "\n",
    "    model_select = sg.Column(checkbox_ls, scrollable=True, key='MODEL_SELECT', expand_x=True)\n",
    "    ok_button = sg.Button('Ok', key='OK_SELECT_MODELS', font=('Calibri', 8), expand_x=True)\n",
    "    exit = sg.Button('Exit', key='EXIT', font=('Calibri', 8), button_color='red', expand_x=True)\n",
    "    month_selector_start = sg.Combo(['Jan','Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n",
    "                              'Oct', 'Nov', 'Dec'], enable_events = True, default_value=None, key='MONTH_SELECT_START', font=('Calibri', 8), expand_x=False)\n",
    "    month_selector_end = sg.Combo(['Jan','Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n",
    "                               'Oct', 'Nov', 'Dec'], enable_events = True, default_value=None, key='MONTH_SELECT_END', font=('Calibri', 8), expand_x=False)\n",
    "    year_selector_start = sg.Combo(list(range(1990, dt.datetime.today().year+1, 1)), enable_events = True, default_value=None, key='YEAR_SELECT_START', font=('Calibri', 8), expand_x=False)\n",
    "    year_selector_end = sg.Combo(list(range(1990, dt.datetime.today().year+1, 1)), enable_events = True, default_value=None, key='YEAR_SELECT_END', font=('Calibri', 8), expand_x=False)\n",
    "    \n",
    "    output_path = [sg.Text(\"Output Folder:\", font=('Calibri Bold', 8)), sg.Input(key='-INPUT-'), sg.FolderBrowse(target='-INPUT-', font=('Calibri', 8), key='OUTPUT_PATH', tooltip='Click to select the destination folder for all data outputs.')] # Target with key '-INPUT-'\n",
    "\n",
    "    section1 = [[model_explanations]]\n",
    "    \n",
    "    layout = [\n",
    "        [back_button],\n",
    "        [title_txt],\n",
    "        [instructions_txt],\n",
    "        [sg.Text(\"Start date of training:\", font=('Calibri Bold', 8)), month_selector_start, year_selector_start, sg.Text(\"End date of training:\", font=('Calibri Bold', 8)), month_selector_end, year_selector_end],\n",
    "        [output_path],\n",
    "        [sg.T(SYMBOL_UP, enable_events=True, k='-OPEN SEC1-', text_color='white'), sg.T('Open/Close Model Guide', font = ('Calibri', 8), enable_events=True, text_color='white', k='-OPEN SEC1-TEXT-')],\n",
    "        [Collapsible(section1, '-SEC1-', collapsed=True)],\n",
    "        [model_select],\n",
    "        [ok_button, exit]\n",
    "    ]\n",
    "    window = sg.Window('Economic Activity Index: Select Model(s)', layout, grab_anywhere=True,resizable=True, font=('Calibri',12))\n",
    "    opened1 = False\n",
    "    date_dict = {}\n",
    "    output_path = ''\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == '-OPEN SEC1-':\n",
    "            opened1 = not opened1\n",
    "            window['-OPEN SEC1-'].update(SYMBOL_DOWN if opened1 else SYMBOL_UP)\n",
    "            window['-SEC1-'].update(visible=opened1)\n",
    "        if event == 'OK_SELECT_MODELS':\n",
    "            if all(v == False for v in values.values()):\n",
    "                sg.Popup('Please select at least one model to use for this analysis.', font=('Calibri', 12), grab_anywhere=True, button_justification='center', title='Error Warning!')\n",
    "            if values['OUTPUT_PATH'] == '':\n",
    "                sg.Popup('Please indicate an output filepath for the generated files.', font=('Calibri', 12), grab_anywhere=True, button_justification='center', title='Error Warning!')\n",
    "            else:\n",
    "                try:\n",
    "                    date_dict['start_date'] = dt.datetime.strptime(values['MONTH_SELECT_START'] + str(values['YEAR_SELECT_START']), '%b%Y').strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    date_dict['start_date'] = ''\n",
    "                try:\n",
    "                    date_dict['end_date'] = dt.datetime.strptime(values['MONTH_SELECT_END'] + str(values['YEAR_SELECT_END']), '%b%Y').strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    date_dict['end_date'] = ''\n",
    "                output_path = values['OUTPUT_PATH']\n",
    "                break\n",
    "        elif event == 'BACK_SELECT_INDICATORS':\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED or event == 'EXIT':\n",
    "            break\n",
    "    window.close()\n",
    "    return event, values, date_dict, output_path "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493de69e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8895ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function used to draw figures for plotting charts\n",
    "def draw_figure(canvas, figure, loc=(0, 0)):\n",
    "    figure_canvas_agg = FigureCanvasTkAgg(figure, canvas)\n",
    "    figure_canvas_agg.draw()\n",
    "    figure_canvas_agg.get_tk_widget().pack(side='top', fill='both', expand=1)\n",
    "    return figure_canvas_agg\n",
    "\n",
    "#Helper function to delete drawn figures\n",
    "def delete_figure_agg(figure_agg):\n",
    "    figure_agg.get_tk_widget().forget()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d9e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the names of all indicator series in the excel file\n",
    "def get_all_series_names(fpath, sheet):\n",
    "    \n",
    "    #Monthly sheet: MonthData\n",
    "    #Quarterly sheet: QuarterlyData\n",
    "    \n",
    "    df = pd.read_excel(fpath, sheet_name = sheet)\n",
    "    return list(df.columns)[1:]\n",
    "\n",
    "#Retrieve a particular series as a pandas dataframe\n",
    "def get_series(fpath, series_name, sheet):\n",
    "    \n",
    "    #Monthly sheet: MonthData\n",
    "    #Quarterly sheet: QuarterlyData\n",
    "    \n",
    "    df = pd.read_excel(fpath, sheet_name = sheet)\n",
    "    df = df[['date', series_name]].dropna()\n",
    "    try:\n",
    "        df['date'] = pd.to_datetime(df['date'], unit='D', origin='1899-12-30')\n",
    "        df['date'] = df['date'].apply(lambda dt: dt.replace(day=1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b4d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information on each indicator series with regards to their particular processing requirements\n",
    "def get_series_information(fpath, series_name, sheet):\n",
    "    if sheet == 'MonthData':\n",
    "        info_sheet = 'InfoM'\n",
    "    else:\n",
    "        info_sheet = 'InfoQ'\n",
    "    df = pd.read_excel(fpath, sheet_name = info_sheet)\n",
    "    df = df.T\n",
    "    new_header = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df.columns = new_header\n",
    "    new_dict = df.to_dict()\n",
    "    return new_dict[series_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Popup screen when saving a chart\n",
    "def save_chart_popup(series_name):\n",
    "    title_text = sg.Text('Save Series Chart: '+ series_name, font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    browse_input = sg.Input('Filepath...', key = 'FILEPATH', disabled = True, font=('Calibri',12))\n",
    "    browse_button = sg.FileSaveAs('Browse', enable_events=True, key='BROWSE', font=('Calibri', 8), expand_x=True, file_types = (('PNG','.png'),('JPEG','.jpg'),('PDF','.pdf'),), tooltip='Input the filename and select the destination folder for the chart.')\n",
    "    save_button = sg.Button('Save', key='SAVE_AS', font=('Calibri', 8), expand_x=True)\n",
    "    cancel_button = sg.Button('Cancel', key='CANCEL', font=('Calibri', 8), button_color='red', expand_x=True)\n",
    "    layout = [\n",
    "        [title_text],\n",
    "        [browse_input, browse_button],\n",
    "        [save_button, cancel_button]\n",
    "    ]\n",
    "    window = sg.Window('Economic Activity Index: Save Series Chart', layout, finalize = True, grab_anywhere=True,resizable=True, font=('Calibri',12))\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'SAVE_AS':\n",
    "            if values['BROWSE'] == '':\n",
    "                sg.Popup('Please indicate a filepath before saving.')\n",
    "            else:\n",
    "                break\n",
    "        elif event == 'CANCEL' or event == WIN.CLOSED:\n",
    "            break\n",
    "    window.close()\n",
    "    return event, values\n",
    "\n",
    "#Popup screen when saving data\n",
    "def save_data_popup(series_name):\n",
    "    title_text = sg.Text('Save Series Dataset: '+ series_name, font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    browse_input = sg.Input('Filepath...', key = 'FILEPATH', disabled = True, font=('Calibri',12))\n",
    "    browse_button = sg.FileSaveAs('Browse', enable_events=True, key='BROWSE', font=('Calibri', 8), expand_x=True, file_types = (('CSV','.csv'),), tooltip='Input the filename and select the destination folder for the data series.')\n",
    "    save_button = sg.Button('Save', key='SAVE_AS', font=('Calibri', 8), expand_x=True)\n",
    "    cancel_button = sg.Button('Cancel', key='CANCEL', font=('Calibri', 8), button_color='red', expand_x=True)\n",
    "    layout = [\n",
    "        [title_text],\n",
    "        [browse_input, browse_button],\n",
    "        [save_button, cancel_button]\n",
    "    ]\n",
    "    window = sg.Window('Economic Activity Index: Save Series Data', layout, finalize = True, grab_anywhere=True,resizable=True, font=('Calibri',12))\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'SAVE_AS':\n",
    "            if values['BROWSE'] == '':\n",
    "                sg.Popup('Please indicate a filepath before saving.')\n",
    "            else:\n",
    "                break\n",
    "        elif event == 'CANCEL' or event == WIN.CLOSED:\n",
    "            break\n",
    "    window.close()\n",
    "    return event, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592eeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open inspect processed data GUI screen. Allows the user to inspect a data series after processing the data based on their selections.\n",
    "def UI_inspect_processed_data(df, info_dict):\n",
    "    \n",
    "    ### Process dataframe\n",
    "    \n",
    "    series_name = info_dict['SERIES_SELECT']\n",
    "    processing_txt_ls = []\n",
    "    \n",
    "    if info_dict['DIFF'] == True:\n",
    "        processing_txt_ls.append(sg.Text('- Month on Month Differential', justification = 'left', font=('Calibri',8)))\n",
    "        if info_dict['FREQUENCY_SELECT'] == 'Monthly':\n",
    "            df[series_name] = df[series_name].pct_change(1)\n",
    "        else:\n",
    "            pass\n",
    "    if info_dict['YEAR'] == True:\n",
    "        processing_txt_ls.append(sg.Text('- Year on Year Growth', justification = 'left', font=('Calibri',8)))\n",
    "        if info_dict['FREQUENCY_SELECT'] == 'Monthly':\n",
    "            df[series_name] = df[series_name].pct_change(12)\n",
    "        else:\n",
    "            df[series_name] = df[series_name].pct_change(4)\n",
    "        df = df.dropna()\n",
    "    if info_dict['NORM'] == True:\n",
    "        processing_txt_ls.append(sg.Text('- Normalize', justification = 'left', font=('Calibri',8)))\n",
    "        new_df = df[df.columns[1:]]\n",
    "        normalized_df=(new_df-new_df.mean())/new_df.std()\n",
    "        normalized_df.insert(0, 'date', df['date'])\n",
    "        df = normalized_df.copy()\n",
    "    \n",
    "    if processing_txt_ls == []:\n",
    "        processing_txt_ls.append(sg.Text('No Change in Data', justification = 'left', font = ('Calibri',8)))\n",
    "    ###\n",
    "    \n",
    "    back_button = sg.Button('Back', key='BACK_INSPECT_DATA', font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Post Processed Data Inspection: '+series_name, font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    intro_txt = sg.Text('The following chart was derived after processing the original dataset based on your chosen processing steps.', font=('Calibri', 12), expand_x=True, justification='center')\n",
    "    download_chart_button = sg.Button('Save Chart', enable_events=True, key='SAVE_CHART', font=('Calibri', 8), expand_x=True)\n",
    "    save_data_button = sg.Button('Save Data', enable_events=True, key='SAVE_DATA', font=('Calibri', 8), expand_x=True)\n",
    "    \n",
    "    plt.plot(list(df['date']), list(df[series_name]), color='blue', linewidth=0.5)\n",
    "    plt.title(series_name+ ' Processed', fontsize = 8)\n",
    "    plt.xlabel('Date', fontsize=6)\n",
    "    plt.ylabel(series_name, fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.grid(True)\n",
    "    fig = plt.gcf()      # if using Pyplot then get the figure from the plot\n",
    "    figure_x, figure_y, figure_w, figure_h = fig.bbox.bounds\n",
    "    \n",
    "    column_layout2 = [[sg.Canvas(size=(figure_w, figure_h), key='canvas')],\n",
    "                      [save_data_button, download_chart_button]]\n",
    "    \n",
    "    describe_dict = df.describe().to_dict()\n",
    "    table_elements = [\n",
    "        ['First Available Date', str(describe_dict['date']['min'].date())],\n",
    "        ['Latest Available Date', str(describe_dict['date']['max'].date())],\n",
    "        ['Value Count', str(round(describe_dict[series_name]['count'],2))],\n",
    "        ['Mean', str(round(describe_dict[series_name]['mean'],2))],\n",
    "        ['Minimum', str(round(describe_dict[series_name]['min'],2))],\n",
    "        ['25th Percentile', str(round(describe_dict[series_name]['25%'],2))],\n",
    "        ['50th Percentile', str(round(describe_dict[series_name]['50%'],2))],\n",
    "        ['75th Percentile', str(round(describe_dict[series_name]['75%'],2))],\n",
    "        ['Maximum', str(round(describe_dict[series_name]['max'],2))],\n",
    "        ['Standard Deviation', str(round(describe_dict[series_name]['std'],2))],\n",
    "    ]\n",
    "    table_headers = ['Statistic', series_name]\n",
    "    \n",
    "    \n",
    "    column_layout3 = [\n",
    "        [sg.Text('Summary Statistics', font=('Calibri Bold',10))],\n",
    "        [sg.Table(table_elements, headings=table_headers, cols_justification=['c','c'], auto_size_columns=True, expand_y=True, font=('Calibri',8), header_font=('Calibri Bold',10))],\n",
    "        [sg.Text('Selected Processing Steps', font=('Calibri Bold',10))]\n",
    "    ]\n",
    "    for t in processing_txt_ls:\n",
    "        column_layout3.append([t])\n",
    "    \n",
    "    layout = [\n",
    "        [back_button],\n",
    "        [title_txt],\n",
    "        [intro_txt],\n",
    "        [sg.Column(column_layout2, element_justification='center'), sg.Column(column_layout3, element_justification='center')],\n",
    "    ]\n",
    "    \n",
    "    window = sg.Window('Economic Activity Index: Inspect Data', layout, location = (0,0), finalize = True, grab_anywhere=True,resizable=True)\n",
    "    fig_canvas_agg = draw_figure(window['canvas'].TKCanvas, fig)\n",
    "    \n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'BACK_INDICATOR_SELECT':\n",
    "            break\n",
    "        if event == 'SAVE_DATA':\n",
    "            save_event, save_values = save_data_popup(series_name)\n",
    "            if save_event == 'SAVE_AS':\n",
    "                fpath = save_values['BROWSE']\n",
    "                try:\n",
    "                    df.to_csv(fpath)\n",
    "                except:\n",
    "                    df.to_excel(fpath)\n",
    "        if event == 'SAVE_CHART':\n",
    "            save_event, save_values = save_chart_popup(series_name)\n",
    "            if save_event == 'SAVE_AS':\n",
    "                plt.savefig(save_values['BROWSE'])\n",
    "        elif event == 'FREQUENCY_SELECT' or event == 'SERIES_SELECT':\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED or event == 'BACK_INSPECT_DATA':\n",
    "            event = 'BACK_INSPECT_DATA'\n",
    "            break\n",
    "    plt.close()\n",
    "    window.close()\n",
    "    return event, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64129c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens GUI screen to inspect a particular data series\n",
    "def UI_inspect_data(fpath, series_name, current_frequency):\n",
    "\n",
    "    if current_frequency == 'Monthly':\n",
    "        sheet = 'MonthData'\n",
    "    else:\n",
    "        sheet = 'QuarterlyData'\n",
    "    \n",
    "    if series_name == '':\n",
    "        series_name = get_all_series_names(fpath, sheet)[0]\n",
    "        df = get_series(fpath, series_name, sheet)\n",
    "        default_info = get_series_information(fpath, series_name, sheet)\n",
    "    elif series_name not in get_all_series_names(fpath, sheet):\n",
    "        series_name = get_all_series_names(fpath, sheet)[0]\n",
    "        df = get_series(fpath, series_name, sheet)\n",
    "        default_info = get_series_information(fpath, series_name, sheet)\n",
    "    else:\n",
    "        df = get_series(fpath, series_name, sheet)\n",
    "        default_info = get_series_information(fpath, series_name, sheet)\n",
    "    new_dict = default_info.copy()\n",
    "    \n",
    "    back_button = sg.Button('Back', key='BACK_INDICATOR_SELECT', font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Data Inspection: '+series_name, font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    intro_txt = sg.Text('Use this screen to inspect a selected indicator from the provided data file. \\nYou may also alter the pre-processing procedures for this indicator prior to running the machine learning framework.', font=('Calibri', 12), expand_x=True, justification='center')\n",
    "    download_chart_button = sg.Button('Save Chart', enable_events=True, key='SAVE_CHART', font=('Calibri', 8), expand_x=True)\n",
    "    save_data_button = sg.Button('Save Data', enable_events=True, key='SAVE_DATA', font=('Calibri', 8), expand_x=True)\n",
    "    #download_chart_input = sg.Input(key='SAVE_CHART_INPUT')\n",
    "    exit = sg.Button('Exit', key='EXIT', font=('Calibri', 8), button_color='red', expand_x=True)\n",
    "    \n",
    "    plt.plot(list(df['date']), list(df[series_name]), color='blue', linewidth=0.5)\n",
    "    plt.title(series_name+ ' ('+sheet+')', fontsize = 8)\n",
    "    plt.xlabel('Date', fontsize=6)\n",
    "    plt.ylabel(series_name, fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "    #f = pd.date_range(list(df['date'])[0], list(df['date'])[-1],20) #these will be x-axis labels\n",
    "    #plt.xticks(f, fontsize=6, rotation=45)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.grid(True)\n",
    "    #plt.figtext(0.1,0.5, df.describe().to_string())\n",
    "    fig = plt.gcf()      # if using Pyplot then get the figure from the plot\n",
    "    figure_x, figure_y, figure_w, figure_h = fig.bbox.bounds\n",
    "    \n",
    "    frequency_selector = sg.Combo(['Monthly','Quarterly'], enable_events = True, default_value=current_frequency, key='FREQUENCY_SELECT', font=('Calibri', 8), expand_x=True, size=(15,1))\n",
    "    series_selector = sg.Combo(get_all_series_names(fpath, sheet), enable_events = True, default_value = series_name, key = 'SERIES_SELECT', font=('Calibri', 8), expand_x=True, size=(15,1))\n",
    "    category_selector = sg.Combo(['target_variable','consumption','exo','financial','government','investment','trade'], default_value = default_info['Category'], key='CATEGORY', font=('Calibri', 8), expand_x=True, size=(15,1))\n",
    "    include_bool = True if default_info['include'] == 1 else False\n",
    "    include_check = sg.Checkbox('Include in Model(s)', default=include_bool, key='INCLUDE', font=('Calibri',8))\n",
    "    diff_bool = True if default_info['diff'] == 1 else False\n",
    "    diff_check = sg.Checkbox('Month on Month Differential', default=diff_bool, key='DIFF', font=('Calibri',8))\n",
    "    year_bool = True if default_info['year'] == 1 else False\n",
    "    year_check = sg.Checkbox('Year on Year Change', default=year_bool, key='YEAR', font=('Calibri',8))\n",
    "    norm_bool = True if default_info['norm'] == 1 else False\n",
    "    norm_check = sg.Checkbox('Normalize', default=norm_bool, key='NORM', font=('Calibri',8))\n",
    "    inspect_processed_button = sg.Button('Inspect Processed Data', enable_events=True, key='INSPECT_PROCESSED_DATA', font=('Calibri', 8), expand_x=True, tooltip=\"Select to view the visualization and summary statistics of the processed data based on the pre-processing steps selected.\")\n",
    "    \n",
    "    column_layout1 = [\n",
    "                      [sg.Text('Data Options', font=('Calibri Bold',10), justification='center')],\n",
    "                      [sg.Text('Select Frequency:', font = ('Calibri',8), justification='left'), frequency_selector],\n",
    "                      [sg.Text('Select Series:', font = ('Calibri',8), justification='left'), series_selector],\n",
    "                      [sg.Text('Data Category:', font = ('Calibri',8), justification='left'), category_selector],\n",
    "                      [include_check],\n",
    "                      [diff_check],\n",
    "                      [year_check],\n",
    "                      [norm_check],\n",
    "                      [inspect_processed_button]\n",
    "                ]\n",
    "    \n",
    "    \n",
    "    column_layout2 = [[sg.Canvas(size=(figure_w, figure_h), expand_x=True, expand_y=True, key='canvas')],\n",
    "                      [save_data_button, download_chart_button]]\n",
    "    \n",
    "    describe_dict = df.describe().to_dict()\n",
    "    table_elements = [\n",
    "        ['First Available Date', str(describe_dict['date']['min'].date())],\n",
    "        ['Latest Available Date', str(describe_dict['date']['max'].date())],\n",
    "        ['Value Count', str(round(describe_dict[series_name]['count'],2))],\n",
    "        ['Mean', str(round(describe_dict[series_name]['mean'],2))],\n",
    "        ['Minimum', str(round(describe_dict[series_name]['min'],2))],\n",
    "        ['25th Percentile', str(round(describe_dict[series_name]['25%'],2))],\n",
    "        ['50th Percentile', str(round(describe_dict[series_name]['50%'],2))],\n",
    "        ['75th Percentile', str(round(describe_dict[series_name]['75%'],2))],\n",
    "        ['Maximum', str(round(describe_dict[series_name]['max'],2))],\n",
    "        ['Standard Deviation', str(round(describe_dict[series_name]['std'],2))],\n",
    "    ]\n",
    "    table_headers = ['Statistic', series_name]\n",
    "    column_layout3 = [\n",
    "        [sg.Text('Summary Statistics', font=('Calibri Bold',10))],\n",
    "        [sg.Table(table_elements, headings=table_headers, cols_justification=['c','c'], auto_size_columns=True, expand_y=True, font=('Calibri',8), header_font=('Calibri Bold',10))]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    layout = [\n",
    "        [back_button],\n",
    "        [title_txt],\n",
    "        [intro_txt],\n",
    "        [sg.Column(column_layout1, element_justification='left'), sg.Column(column_layout2, element_justification='center'), sg.Column(column_layout3, element_justification='center')],\n",
    "        [exit]\n",
    "    ]\n",
    "    \n",
    "    window = sg.Window('Economic Activity Index: Inspect Data', layout, location = (0,0), finalize = True, grab_anywhere=True,resizable=True)\n",
    "    fig_canvas_agg = draw_figure(window['canvas'].TKCanvas, fig)\n",
    "    \n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event == 'BACK_INDICATOR_SELECT':\n",
    "            break\n",
    "        if event == 'INSPECT_PROCESSED_DATA':\n",
    "            break\n",
    "            new_event, new_values = UI_inspect_processed_data(df, values)\n",
    "        if event == 'SAVE_DATA':\n",
    "            save_event, save_values = save_data_popup(series_name)\n",
    "            if save_event == 'SAVE_AS':\n",
    "                fpath = save_values['BROWSE']\n",
    "                try:\n",
    "                    df.to_csv(fpath)\n",
    "                except:\n",
    "                    df.to_excel(fpath)\n",
    "        if event == 'SAVE_CHART':\n",
    "            save_event, save_values = save_chart_popup(series_name)\n",
    "            if save_event == 'SAVE_AS':\n",
    "                plt.savefig(save_values['BROWSE'])\n",
    "        elif event == 'FREQUENCY_SELECT' or event == 'SERIES_SELECT':\n",
    "            break\n",
    "        elif event == sg.WIN_CLOSED or event == 'EXIT':\n",
    "            break\n",
    "    plt.close()\n",
    "    window.close()\n",
    "    try:\n",
    "        new_dict['Category'] = values['CATEGORY']\n",
    "        new_dict['include'] = 1 if values['INCLUDE'] == True else 0\n",
    "        new_dict['diff'] = 1 if values['DIFF'] == True else 0\n",
    "        new_dict['year'] = 1 if values['YEAR'] == True else 0\n",
    "        new_dict['norm'] = 1 if values['NORM'] == True else 0\n",
    "\n",
    "        if default_info == new_dict:\n",
    "            default_info['edited'] = 0\n",
    "        else:\n",
    "            default_info['edited'] = 1\n",
    "    except:\n",
    "        default_info['edited'] = 0\n",
    "    \n",
    "    return event, values, default_info, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c566012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens the GUI screen for the entire inspect data flow. Utilizes all inspect data functions.\n",
    "def inspect_data_flow(fpath):\n",
    "    event, values, info_dict, df = UI_inspect_data(fpath, '', 'Monthly')\n",
    "    while True:\n",
    "        if event == 'SERIES_SELECT' or event == 'FREQUENCY_SELECT' or event == 'BACK_INSPECT_DATA':\n",
    "            event, values, info_dict, df = UI_inspect_data(fpath, values['SERIES_SELECT'], values['FREQUENCY_SELECT'])\n",
    "        if event == 'INSPECT_PROCESSED_DATA':\n",
    "            event, _ = UI_inspect_processed_data(df, values)\n",
    "        elif event == 'BACK_INDICATOR_SELECT':\n",
    "            event = 'BACK_SELECT_INDICATORS'\n",
    "            break\n",
    "        elif event == 'EXIT' or event == None:\n",
    "            event = 'BACK_SELECT_INDICATORS'\n",
    "            break\n",
    "    return event, values, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bb652",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reconfigure Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01568c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from a dictionary of values from the monthly and quarterly information sheets of the excel file.\n",
    "def get_data_from_table(table_values):\n",
    "    key_ls = list(table_values.keys())\n",
    "    new_dict = {'Economic Indicators':[], \n",
    "                'Freq':[], \n",
    "                'Unit':[], \n",
    "                'Source':[], \n",
    "                'Series Name':[], \n",
    "                'Compiled by':[], \n",
    "                'Category':[], \n",
    "                'include':[], \n",
    "                'diff':[], \n",
    "                'year':[],\n",
    "                 'norm': []} #added\n",
    "    \n",
    "    for key in list(table_values.keys())[:-1]:\n",
    "        new_dict[key[:-2]].append(table_values[key])\n",
    "        \n",
    "    return pd.DataFrame(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d824288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates an editable version of the information sheets of the excel on the GUI. Allows users to manipulate the values in that table.\n",
    "def create_editable_table(fpath, frequency):\n",
    "    if frequency == 'Monthly':\n",
    "        sheet = 'InfoM'\n",
    "    else:\n",
    "        sheet = 'InfoQ'\n",
    "    df = pd.read_excel(fpath, sheet_name = sheet)\n",
    "    \n",
    "    cols = list(df.columns)\n",
    "    headers = []\n",
    "    for c in cols:\n",
    "        headers.append(sg.Text(c, pad=(0,0), size=(17,1), font=('Calibri Bold', 8), justification='c'))\n",
    "    \n",
    "    back_button = sg.Button('Back', key='BACK_SELECT_INDICATORS', font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Edit ' + frequency +' Indicator Information', font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    \n",
    "    layout = [[back_button],[title_txt],[headers]]\n",
    "    combo_keys = []\n",
    "    \n",
    "    for row in range(0, len(df)):\n",
    "        layout.append([\n",
    "            sg.Input(df.iloc[row,0], size=(17,1),pad=(0, 0), key='Economic Indicators '+str(row), font=('Calibri',8), disabled=True, justification='center'), \n",
    "            sg.Input(df.iloc[row,1], size=(17,1),pad=(0, 0), key='Freq '+str(row), font=('Calibri',8), disabled=True, justification='center'), \n",
    "            sg.Input(df.iloc[row,2], size=(17,1),pad=(0, 0), key='Unit '+str(row), font=('Calibri',8), disabled=True, justification='center'),\n",
    "            sg.Input(df.iloc[row,3], size=(17,1),pad=(0, 0), key='Source '+str(row), font=('Calibri',8), disabled=True, justification='center'), \n",
    "            sg.Input(df.iloc[row,4], size=(17,1),pad=(0, 0), key='Series Name '+str(row), font=('Calibri',8), disabled=True, justification='center'),\n",
    "            sg.Input(df.iloc[row,5], size=(17,1),pad=(0, 0), key='Compiled by '+str(row), font=('Calibri',8), disabled=True, justification='center'), \n",
    "            sg.Combo(['target_variable','consumption','exo','financial','government','investment','trade'], default_value= df.iloc[row,6], size=(16,1),pad=(0, 0), key='Category '+str(row), font=('Calibri',8)),\n",
    "            sg.Combo([1,0], default_value=int(df.iloc[row,7]), size=(15,1),pad=(0, 0), key='include '+str(row), font=('Calibri',8)),\n",
    "            sg.Combo([1,0], default_value=int(df.iloc[row,8]), size=(15,1),pad=(0, 0), key='diff '+str(row), font=('Calibri',8)),\n",
    "            sg.Combo([1,0], default_value=int(df.iloc[row,9]), size=(15,1),pad=(0, 0), key='year '+str(row), font=('Calibri',8)),\n",
    "            sg.Combo([1,0], default_value=int(df.iloc[row,10]), size=(15,1),pad=(0, 0), key='norm '+str(row), font=('Calibri',8))\n",
    "        ])\n",
    "        combo_keys = combo_keys + ['Category '+str(row),'include '+str(row),'diff '+str(row),'year '+str(row), 'norm '+str(row)]\n",
    "        \n",
    "    layout.append([sg.Button(\"Submit\", key = 'SUBMIT_SPREADSHEET', font=('Calibri',8)),\n",
    "                   sg.Button(\"Revert to Original\", key = 'REVERT_ORIGINAL', font=('Calibri',8), tooltip=\"Select to revert back to the original file's inputs\"),\n",
    "                   sg.Checkbox('Save Changes to Working File', key = 'SAVE_CHANGES', font=('Calibri', 8), expand_x = False,default=False, tooltip = 'Select to overwrite the modifications made to the Excel Template.')])\n",
    "    window = sg.Window('Spreadsheet', layout, font=('Calibri',12), resizable=True, grab_anywhere=True,finalize=True)\n",
    "    for combo in combo_keys:\n",
    "        window[combo].Widget.configure(justify='center')\n",
    "\n",
    "\n",
    "    while True:\n",
    "        event, values = window.read()\n",
    "        if event in (sg.WIN_CLOSED, 'EXIT'):\n",
    "            event = 'BACK_SELECT_INDICATORS'\n",
    "            break\n",
    "        if event == 'BACK_SELECT_INDICATORS':\n",
    "            break\n",
    "        elif event == 'REVERT_ORIGINAL':\n",
    "            break\n",
    "        elif event == 'SUBMIT_SPREADSHEET':\n",
    "            new_table = get_data_from_table(values)\n",
    "            window.close()\n",
    "            event = 'BACK_SELECT_INDICATORS'\n",
    "            if new_table.equals(df):\n",
    "                return event, values, df\n",
    "            else:\n",
    "                if values['SAVE_CHANGES'] == True:\n",
    "                    with pd.ExcelWriter(fpath, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: #added\n",
    "                        new_table.to_excel(writer, sheet_name=sheet, index=False)\n",
    "                return event, values, new_table\n",
    "        \n",
    "    window.close()\n",
    "    return event, values, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b38b8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Popup to allow users to choose which frequency information they would like to inspect.\n",
    "def frequency_popup():\n",
    "    choice, _ = sg.Window('Economic Activity Index: Choose Data Frequency', [[sg.T(\"Choose a Data Frequency to Edit\", font=('Calibri Bold',12), justification='center')], [sg.Button('Monthly', font=('Calibri',8), expand_x=True), sg.Button('Quarterly', font=('Calibri',8), expand_x=True)]], disable_close=False).read(close=True)\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ba02c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52744d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final process flow utilizing all previous functions. This function is ran before any back end functions are used.\n",
    "def UI_process_flow():\n",
    "    event = 'START'\n",
    "    model_selection = {}\n",
    "    added_data_dict = {}\n",
    "    added_data = {}\n",
    "    fpath = ''\n",
    "    monthly_info = ''\n",
    "    quarterly_info = ''\n",
    "    frequency = 'Monthly'\n",
    "    sg.theme('DarkBlue13')\n",
    "    \n",
    "    while True:\n",
    "        if event == 'START' or event == 'BACK_HOME':\n",
    "            event = UI_home_page_design()\n",
    "        if event == 'CONTINUE_HOME' or event == 'BACK_CHECK_EXCEL':\n",
    "            event, all_data, fpath = UI_add_excel_data()\n",
    "            if fpath != '':\n",
    "                monthly_info = pd.read_excel(fpath, sheet_name = 'InfoM')\n",
    "                quarterly_info = pd.read_excel(fpath, sheet_name = 'InfoQ')\n",
    "        if event == 'SUBMIT_EXCEL' or event == 'BACK_SELECT_INDICATORS':\n",
    "            event, values, added_data = UI_choose_indicators(all_data)\n",
    "            added_data_dict = added_data \n",
    "        if event == 'INSPECT_INDICATORS':\n",
    "            event, values, info_dict = inspect_data_flow(fpath)\n",
    "        if event == 'RECONFIGURE_INDICATORS':\n",
    "            frequency = frequency_popup()\n",
    "            if frequency == None:\n",
    "                frequency = 'Monthly'\n",
    "            event, values, df = create_editable_table(fpath, frequency)\n",
    "            if frequency == 'Monthly':\n",
    "                monthly_info = df.copy()\n",
    "            else:\n",
    "                quarterly_info = df.copy()\n",
    "        if event == 'REVERT_ORIGINAL':\n",
    "            event, values, df = create_editable_table(fpath, frequency)\n",
    "        if event == 'OK_CHOOSE_INDICATORS':\n",
    "            event, model_selection, date_dict, output_path = UI_select_model()\n",
    "        elif event == 'OK_SELECT_MODELS':\n",
    "            return event, model_selection, added_data_dict, date_dict, monthly_info, quarterly_info, output_path\n",
    "        elif event == sg.WIN_CLOSED or event == 'EXIT':\n",
    "            return {}, {}, {}, {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38fd66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Back end Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8107a26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5bdbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper classes and functions\n",
    "#The hidden prints class hides any string prints from a function\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ac73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use data outputs from UI to organize the information for each series\n",
    "\n",
    "def separate_indicator_target_data(added_data_dict, monthly_info, quarterly_info):\n",
    "    var_dict = {\n",
    "                 'target_variable': {},\n",
    "                 'consumption': {},\n",
    "                 'exo': {},\n",
    "                 'financial': {},\n",
    "                 'government': {},\n",
    "                 'investment': {},\n",
    "                 'trade': {}}\n",
    "    q_d = quarterly_info.T.to_dict()\n",
    "    m_d = monthly_info.T.to_dict()\n",
    "    for key in q_d.keys():\n",
    "        try:\n",
    "            new_dict = {}\n",
    "            new_dict['frequency'] = q_d[key]['Freq']\n",
    "            new_dict['include'] = q_d[key]['include']\n",
    "            new_dict['diff'] = q_d[key]['diff']\n",
    "            new_dict['year'] = q_d[key]['year']\n",
    "            new_dict['norm'] = q_d[key]['norm']\n",
    "            new_dict['data'] = added_data_dict[q_d[key]['Economic Indicators']]['Data']\n",
    "            try:\n",
    "                new_dict['data']['date'] = pd.to_datetime(new_dict['data']['date'], unit='D', origin='1899-12-30')\n",
    "                new_dict['data']['date'] = new_dict['data']['date'].apply(lambda dt: dt.replace(day=1))\n",
    "            except:\n",
    "                pass\n",
    "            var_dict[q_d[key]['Category']][q_d[key]['Economic Indicators']] = new_dict\n",
    "        except:\n",
    "            pass\n",
    "    for key in m_d.keys():\n",
    "        try:\n",
    "            new_dict = {}\n",
    "            new_dict['frequency'] = m_d[key]['Freq']\n",
    "            new_dict['include'] = m_d[key]['include']\n",
    "            new_dict['diff'] = m_d[key]['diff']\n",
    "            new_dict['year'] = m_d[key]['year']\n",
    "            new_dict['norm'] = m_d[key]['norm']\n",
    "            new_dict['data'] = added_data_dict[m_d[key]['Economic Indicators']]['Data']\n",
    "            try:\n",
    "                new_dict['data']['date'] = pd.to_datetime(new_dict['data']['date'], unit='D', origin='1899-12-30')\n",
    "                new_dict['data']['date'] = new_dict['data']['date'].apply(lambda dt: dt.replace(day=1))\n",
    "            except:\n",
    "                pass\n",
    "            var_dict[m_d[key]['Category']][m_d[key]['Economic Indicators']] = new_dict\n",
    "        except:\n",
    "            pass\n",
    "    return var_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f070f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "043069e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP column (target variable) processing\n",
    "\n",
    "'''\n",
    "- get data\n",
    "- forecast missing values\n",
    "- calculate filters, norm, sbbq\n",
    "- get all stats\n",
    "- splice data if multiple targets are included\n",
    "- perform relevant processing steps\n",
    "'''\n",
    "\n",
    "\n",
    "def process_target_data(var_dict):\n",
    "    \n",
    "    target_dict = var_dict['target_variable']\n",
    "    target_name = list(target_dict.keys())[0]\n",
    "    df = target_dict[target_name]['data'].copy()\n",
    "\n",
    "    if target_dict[target_name]['diff'] == True:\n",
    "        if target_dict[target_name]['frequency'] == 'M':\n",
    "            df[target_name] = df[target_name].pct_change(1)\n",
    "        else:\n",
    "            pass\n",
    "    if target_dict[target_name]['year'] == True:\n",
    "        if target_dict[target_name]['frequency'] == 'M':\n",
    "            df[target_name] = df[target_name].pct_change(12)\n",
    "        else:\n",
    "            df[target_name] = df[target_name].pct_change(4)\n",
    "        df = df.dropna()\n",
    "        \n",
    "    if target_dict[target_name]['norm'] == True:\n",
    "        new_df = df[df.columns[1:]]\n",
    "        normalized_df=(new_df-new_df.mean())/new_df.std()\n",
    "        normalized_df.insert(0, 'date', df['date'])\n",
    "        df = normalized_df.copy()\n",
    "    \n",
    "    original_target = list(df[target_name])\n",
    "    #convert to gdp growth gap\n",
    "    mean_g_growth = statistics.mean([x for x in list(df[target_name]) if str(x) != 'nan'])\n",
    "    df['GDP_Gap'] = df[target_name].apply(lambda x: x - mean_g_growth)\n",
    "    df[target_name] = original_target\n",
    "\n",
    "    return df[['date','GDP_Gap']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a50d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates all relevant filters for the target variable\n",
    "def calculate_filters(df):\n",
    "    gdp_ls = list(df['GDP_Gap'])\n",
    "    new_gdp_ls = [x for x in gdp_ls if str(x) != 'nan']\n",
    "    g_hp, _ = sm.tsa.filters.hpfilter(new_gdp_ls, 1600)\n",
    "    g_cf, _ = sm.tsa.filters.cffilter(new_gdp_ls, 8, 32)\n",
    "    g_ham_cycle, g_ham_trend = quantecon.hamilton_filter(new_gdp_ls, h=8, p=4)\n",
    "    \n",
    "    filter_dict = {'g_hp': [], 'g_cf': [], 'g_ham_cycle': [], 'g_ham_trend': []}\n",
    "    filter_dict_init = {'g_hp': g_hp, 'g_cf': g_cf, 'g_ham_cycle': g_ham_cycle, 'g_ham_trend': g_ham_trend}\n",
    "    count = 0\n",
    "    for i in range(len(gdp_ls)):\n",
    "        if math.isnan(gdp_ls[i]):\n",
    "            for key in filter_dict.keys():\n",
    "                filter_dict[key].append(np.nan)\n",
    "        else:\n",
    "            for key in filter_dict.keys():\n",
    "                filter_dict[key].append(filter_dict_init[key][count])\n",
    "            count += 1\n",
    "    new_df = df.copy()        \n",
    "    for key  in filter_dict.keys():\n",
    "        new_df[key] = filter_dict[key]\n",
    "    new_df['date'] = pd.PeriodIndex(new_df['date'], freq='Q')\n",
    "    new_df = new_df.rename(columns={'date': 'quarter'})\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the normalized values of the GDP gap variable\n",
    "def calculate_norm(df):\n",
    "    col_ls = ['g_hp','g_cf','g_ham_cycle','GDP_Gap'] \n",
    "    for col in col_ls:\n",
    "        d_ls = list(df[col])\n",
    "        new_d_ls = [x for x in d_ls if str(x) != 'nan']\n",
    "        d_mu = statistics.mean(new_d_ls)\n",
    "        d_sd = statistics.stdev(new_d_ls)\n",
    "        norm_ls = []\n",
    "        for i in d_ls:\n",
    "            norm_ls.append((i-d_mu)/d_sd)\n",
    "        df[col+'_norm'] = norm_ls\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870fde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate business cycle information using the TP detection pipeline\n",
    "def calculate_sbbq(df):\n",
    "    col_ls = ['g_hp_norm','g_cf_norm','g_ham_cycle_norm','GDP_Gap_norm'] \n",
    "    new_df = df[col_ls]\n",
    "    new_df = cif.pipelineTPDetection(df = new_df.reset_index(), origColumns = col_ls, printDetails = False, showPlots = False, savePlots = None, saveLogs = None)\n",
    "    for col in col_ls:\n",
    "        df[col+'_point'] = list(new_df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0c1d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cycle statistics for GDP gap metric\n",
    "def compute_total_cycles(point_ls):\n",
    "    non_zero_ls = [x for x in point_ls if x != 0]\n",
    "    count = 0\n",
    "    for i in non_zero_ls:\n",
    "        if i == 1:\n",
    "            count+=1\n",
    "    return count - 1\n",
    "\n",
    "def compute_total_tp(point_ls):\n",
    "    non_zero_ls = [x for x in point_ls if x != 0]\n",
    "    non_zero_ls = [x for x in non_zero_ls if str(x) != 'nan']\n",
    "    return len(non_zero_ls)\n",
    "\n",
    "def compute_avg_slowdown_duration(point_ls):\n",
    "    duration_ls = []\n",
    "    for i in range(len(point_ls)):\n",
    "        if point_ls[i] == 1:\n",
    "            j = i\n",
    "            count = 1\n",
    "            while j < len(point_ls) and point_ls[j] != -1:\n",
    "                count+=1\n",
    "                j+=1\n",
    "            duration_ls.append(count)\n",
    "    try:\n",
    "        mean_dur = statistics.mean(duration_ls)\n",
    "    except:\n",
    "        mean_dur = 0\n",
    "    try:\n",
    "        std_dur = statistics.stdev(duration_ls)\n",
    "    except:\n",
    "        std_dur = 0\n",
    "    try:\n",
    "        div = std_dur/mean_dur\n",
    "    except:\n",
    "        div = 0\n",
    "    return mean_dur, div\n",
    "\n",
    "def compute_avg_expansion_duration(point_ls):\n",
    "    duration_ls = []\n",
    "    for i in range(len(point_ls)):\n",
    "        if point_ls[i] == -1:\n",
    "            j = i\n",
    "            count = 1\n",
    "            while j < len(point_ls) and point_ls[j] != 1:\n",
    "                count+=1\n",
    "                j+=1\n",
    "            duration_ls.append(count)\n",
    "    try:\n",
    "        mean_dur = statistics.mean(duration_ls)\n",
    "    except:\n",
    "        mean_dur = 0\n",
    "    try:\n",
    "        std_dur = statistics.stdev(duration_ls)\n",
    "    except:\n",
    "        std_dur = 0\n",
    "    try:\n",
    "        div = std_dur/mean_dur\n",
    "    except:\n",
    "        div = 0\n",
    "    return mean_dur, div\n",
    "\n",
    "def compute_avg_cycle_duration(s_duration, e_duration):\n",
    "    return s_duration + e_duration\n",
    "\n",
    "def compute_avg_slowdown_amplitude(point_ls, data_ls):\n",
    "    amp_ls = []\n",
    "    for i in range(len(point_ls)):\n",
    "        if point_ls[i] == -1:\n",
    "            amp_ls.append(data_ls[i])\n",
    "    try:\n",
    "        mean_amp = statistics.mean(amp_ls)\n",
    "    except:\n",
    "        mean_amp = 0\n",
    "    try:\n",
    "        std_amp = statistics.stdev(amp_ls)\n",
    "    except:\n",
    "        std_amp = 0\n",
    "    try:\n",
    "        div = std_amp/mean_amp\n",
    "    except:\n",
    "        div = 0\n",
    "    return mean_amp, div\n",
    "\n",
    "def compute_avg_expansion_amplitude(point_ls, data_ls):\n",
    "    amp_ls = []\n",
    "    for i in range(len(point_ls)):\n",
    "        if point_ls[i] == 1:\n",
    "            amp_ls.append(data_ls[i])\n",
    "    try:\n",
    "        mean_amp = statistics.mean(amp_ls)\n",
    "    except:\n",
    "        mean_amp = 0\n",
    "    try:\n",
    "        std_amp = statistics.stdev(amp_ls)\n",
    "    except:\n",
    "        std_amp = 0\n",
    "    try:\n",
    "        div = std_amp/mean_amp\n",
    "    except:\n",
    "        div = 0\n",
    "    return mean_amp, div\n",
    "\n",
    "def compute_speeds(e_dur, e_amp, s_dur, s_amp):\n",
    "    try:\n",
    "        expansion_speed = e_amp/e_dur\n",
    "    except:\n",
    "        expansion_speed = 0\n",
    "    try:\n",
    "        slowdown_speed = s_amp/s_dur\n",
    "    except:\n",
    "        slowdown_speed = 0\n",
    "    return expansion_speed, slowdown_speed\n",
    "\n",
    "def compute_last_quarter(df):\n",
    "    new_df = df[df['GDP_Gap_norm_point'].notna()]\n",
    "    qtr_ls = list(new_df['quarter'])\n",
    "    point_ls = list(new_df['GDP_Gap_norm_point'])\n",
    "    latest_qtr_data = qtr_ls[-1]\n",
    "    point_ls.reverse()\n",
    "    qtr_ls.reverse()\n",
    "    try:\n",
    "        last_qtr_peak = qtr_ls[point_ls.index(1)]\n",
    "        num_qtrs_last_peak = point_ls.index(1)+1\n",
    "    except:\n",
    "        last_qtr_peak = qtr_ls[point_ls.index(0)]\n",
    "        num_qtrs_last_peak = point_ls.index(0)+1\n",
    "    return {'Latest Quarter with Data': latest_qtr_data, 'Latest Quarter Peak': last_qtr_peak,\n",
    "            'No. of Quarters from Last Peak': num_qtrs_last_peak}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d819c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cycle statistics\n",
    "def calculate_final_stats(df):\n",
    "    final_dict = {}\n",
    "    col_ls = ['g_hp_norm','g_cf_norm','g_ham_cycle_norm','GDP_Gap_norm']\n",
    "    for col in col_ls:\n",
    "        data_ls = list(df[col])\n",
    "        point_ls = list(df[col+'_point'])\n",
    "        total_cycles = compute_total_cycles(point_ls)\n",
    "        total_tps = compute_total_tp(point_ls)\n",
    "        avg_d_slowdown, cv_d_slowdown = compute_avg_slowdown_duration(point_ls)\n",
    "        avg_d_expansion, cv_d_expansion = compute_avg_expansion_duration(point_ls)\n",
    "        avg_d_cycles = compute_avg_cycle_duration(avg_d_slowdown, avg_d_expansion)\n",
    "        avg_a_slowdown, cv_a_slowdown = compute_avg_slowdown_amplitude(point_ls, data_ls)\n",
    "        avg_a_expansion, cv_a_expansion = compute_avg_expansion_amplitude(point_ls, data_ls)\n",
    "        expansion_speed, slowdown_speed = compute_speeds(avg_d_expansion, avg_a_expansion, avg_d_slowdown, avg_a_slowdown)\n",
    "        new_dict = {'Number of Cycles': total_cycles, 'Total No. of TPs': total_tps,\n",
    "                    'Avg. Duration of Cycles': avg_d_cycles, 'Avg. Duration of Slowdown': avg_d_slowdown,\n",
    "                    'Avg. Duration of Expansion': avg_d_expansion, 'Avg. Amplitude of Slowdown': avg_a_slowdown,\n",
    "                    'Avg. Amplitude of Expansion': avg_a_expansion, 'Speed of Slowdown': slowdown_speed,\n",
    "                    'Speed of Expansion': expansion_speed, 'CV of Slowdown Duration': cv_d_slowdown, \n",
    "                    'CV of Expansion Duration': cv_d_expansion, 'CV of Slowdown Amplitude': cv_a_slowdown,\n",
    "                    'CV of Expansion Amplitude': cv_a_expansion}\n",
    "        final_dict[col] = new_dict\n",
    "        final_dict['summ_stat'] = compute_last_quarter(df)\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "930ae538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the preceding cycle functions to get the updated normalized values, filters and cycle statistics for GDP growth gap\n",
    "def get_cycles_information(df):\n",
    "    with HiddenPrints():\n",
    "        df = calculate_filters(df)\n",
    "        df = calculate_norm(df)\n",
    "        df = calculate_sbbq(df)\n",
    "        stat_dict = calculate_final_stats(df)\n",
    "    return df, stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69457d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data processing. Processing steps are dependent on the users choices.\n",
    "\n",
    "'''\n",
    "- prune to selected indicators only\n",
    "- forecast missing end values\n",
    "- log/ln for selected indicators\n",
    "- YoY or MoM differential for selected indicators\n",
    "- normalize for selected indicators\n",
    "- merge data with target variable\n",
    "'''\n",
    "\n",
    "def process_indicator_vars(var_dict):\n",
    "    processed_data_dict = {\n",
    "                 'consumption': [],\n",
    "                 'exo': [],\n",
    "                 'financial': [],\n",
    "                 'government': [],\n",
    "                 'investment': [],\n",
    "                 'trade': []}\n",
    "    \n",
    "    contributions_dict = {\n",
    "                 'consumption': [],\n",
    "                 'exo': [],\n",
    "                 'financial': [],\n",
    "                 'government': [],\n",
    "                 'investment': [],\n",
    "                 'trade': []}\n",
    "    \n",
    "    q_indicator_ls = []\n",
    "    for sector in var_dict.keys():\n",
    "        if sector != 'target_variable':\n",
    "            contributions_dict[sector] = list(var_dict[sector].keys())\n",
    "            for indicator in var_dict[sector].keys():\n",
    "                indicator_dict = var_dict[sector][indicator]\n",
    "                df = indicator_dict['data'].copy()\n",
    "                if indicator_dict['diff'] == True:\n",
    "                    if indicator_dict['frequency'] == 'M':\n",
    "                        df[indicator] = df[indicator].pct_change(1)\n",
    "                    else:\n",
    "                        pass\n",
    "                if indicator_dict['year'] == True:\n",
    "                    if indicator_dict['frequency'] == 'M':\n",
    "                        df[indicator] = df[indicator].pct_change(12)\n",
    "                    else:\n",
    "                        df[indicator] = df[indicator].pct_change(4)\n",
    "                    df = df.dropna()\n",
    "\n",
    "                if indicator_dict['norm'] == True:\n",
    "                    new_df = df[df.columns[1:]]\n",
    "                    normalized_df=(new_df-new_df.mean())/new_df.std()\n",
    "                    normalized_df.insert(0, 'date', df['date'])\n",
    "                    df = normalized_df.copy()\n",
    "                df.columns = ['date',indicator]\n",
    "                if indicator_dict['frequency'] == 'Q':\n",
    "                    df['date'] = pd.PeriodIndex(df['date'], freq='Q')\n",
    "                    df = df.set_index('date').resample('M').interpolate(option='spline')\n",
    "                    df = df.to_timestamp()\n",
    "                    df = df.reset_index()\n",
    "                    missing_actual = list(forecast_missing_vals(df, indicator).values())\n",
    "                    actual_ls = list(df[indicator])\n",
    "                    for i in range(len(actual_ls)):\n",
    "                        if math.isnan(actual_ls[i]):\n",
    "                            actual_ls[i] = missing_actual[i]\n",
    "                    df[indicator] = actual_ls\n",
    "                    #df['date'] = pd.to_datetime(df['date'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                processed_data_dict[sector].append(df)\n",
    "    \n",
    "    data_ls = []\n",
    "    for key in processed_data_dict.keys():\n",
    "        for i in processed_data_dict[key]:\n",
    "            data_ls.append(i)\n",
    "       \n",
    "    final_df = reduce(lambda left, right: pd.merge(left, right, on = ['date'], how = \"outer\"), data_ls)\n",
    "    final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "    final_df = final_df.sort_values(by='date',ascending=True)\n",
    "    \n",
    "    return final_df, contributions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f865a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the target and indicator data series\n",
    "def merge_data(added_data_dict, monthly_info, quarterly_info):\n",
    "    var_dict = separate_indicator_target_data(added_data_dict, monthly_info, quarterly_info)\n",
    "    df = process_target_data(var_dict)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['index'], axis=1)\n",
    "    indicator_df, contributions_dict = process_indicator_vars(var_dict)\n",
    "    cycles_df, stat_dict = get_cycles_information(df)\n",
    "    \n",
    "    cycles_df = cycles_df[['quarter','g_hp_norm','g_cf_norm','g_ham_cycle_norm','GDP_Gap']]\n",
    "    cycles_df.columns = ['quarter','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']\n",
    "    qtr_ls = list(cycles_df['quarter'])\n",
    "    date_ls = []\n",
    "    for q in qtr_ls:\n",
    "        date_ls.append(q.to_timestamp())\n",
    "    end_date = date_ls[-1]  + np.timedelta64(3, 'M')\n",
    "    date_range = list(pd.date_range(date_ls[0], end_date, freq='MS'))\n",
    "    new_dict = {'date': date_range}\n",
    "    cycles_df['date'] = date_ls\n",
    "    for col in ['date','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']:\n",
    "        new_dict[col] = []\n",
    "        col_ls = list(cycles_df[col])\n",
    "        for i in range(len(date_ls)):\n",
    "            new_dict[col] += [col_ls[i],col_ls[i],col_ls[i]]\n",
    "    date_norm_df = pd.DataFrame(new_dict)\n",
    "    while len(list(date_norm_df['date'])) < len(date_range):\n",
    "        del date_range[-1]\n",
    "    date_norm_df['date'] = date_range\n",
    "    date_norm_df = date_norm_df[date_norm_df['date'] >= datetime(1990,1,1)]\n",
    "    \n",
    "    #merge EAI data\n",
    "    date_norm_df['date'] = pd.to_datetime(date_norm_df['date'])\n",
    "    indicator_df['date'] = pd.to_datetime(indicator_df['date'])\n",
    "    if list(date_norm_df['date'])[-1] < list(indicator_df['date'])[-1]:\n",
    "        df_ls = [indicator_df, date_norm_df]\n",
    "    else:\n",
    "        df_ls = [date_norm_df, indicator_df]\n",
    "    data_monthly = reduce(lambda left, right: pd.merge(left, right, on = ['date'], how = \"outer\"), df_ls)\n",
    "    \n",
    "    #create a quarterly version\n",
    "    data_quarterly = data_monthly.copy()\n",
    "    data_quarterly['quarter'] = data_quarterly['date'].dt.to_period('Q')\n",
    "    data_quarterly = data_quarterly.groupby('quarter', as_index=False, sort=False).mean()\n",
    "    data_quarterly = data_quarterly.drop(columns=['date'])\n",
    "    \n",
    "    return data_quarterly, data_monthly, stat_dict, contributions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0ee6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2679b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a null value if the date is greater than start date\n",
    "def add_nans(start_date, date, val):\n",
    "    if date > start_date:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f28e9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast missing values using ARIMA\n",
    "def forecast_missing_vals(df, col):\n",
    "    mod = ARIMA((df[col].astype(float)), order=(1,0,0))\n",
    "    res = mod.fit()\n",
    "    return dict(res.predict(0,len(df[col])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a34b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare all data for training\n",
    "def prep_data(added_data_dict, monthly_info, quarterly_info):\n",
    "    data_quarterly, data_monthly, cycles_dict, contributions_dict = merge_data(added_data_dict, monthly_info, quarterly_info)\n",
    "    old_data_monthly = data_monthly.copy()\n",
    "    old_data_quarterly = data_quarterly.copy()\n",
    "    data_monthly = data_monthly.dropna(axis=1,how='all')\n",
    "    #data_monthly = data_monthly.loc[:, data_monthly.isnull().mean() < .85]\n",
    "    earliest_actual_date = list(data_monthly[['date','GDP_Gap']].dropna()['date'])[0]\n",
    "    latest_actual_date = list(data_monthly[['date','GDP_Gap']].dropna()['date'])[-1]\n",
    "    latest_actual_quarter = list(data_monthly[['date','GDP_Gap']].dropna()['date'])[-3]\n",
    "    data_monthly = data_monthly.drop(columns=['HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap'])\n",
    "    gdp_df = data_quarterly[['quarter','GDP_Gap']].set_index('quarter').resample('M').interpolate(option='spline')\n",
    "    gdp_df = gdp_df.to_timestamp()\n",
    "    gdp_df = gdp_df.reset_index()\n",
    "    gdp_df.columns = ['date','GDP_Gap']\n",
    "    gdp_df = gdp_df[gdp_df['date'] <= latest_actual_date]\n",
    "    gdp_df['GDP_Gap'] = gdp_df.apply(lambda x: add_nans(latest_actual_quarter, x['date'], x['GDP_Gap']),axis=1)\n",
    "    missing_actual = list(forecast_missing_vals(gdp_df, 'GDP_Gap').values())\n",
    "    actual_ls = list(gdp_df['GDP_Gap'])\n",
    "    for i in range(len(actual_ls)):\n",
    "        if math.isnan(actual_ls[i]):\n",
    "            actual_ls[i] = missing_actual[i]\n",
    "    gdp_df['GDP_Gap'] = actual_ls\n",
    "    gdp_df = gdp_df[gdp_df['date'] >= earliest_actual_date]\n",
    "    data_monthly = data_monthly.dropna()\n",
    "    full_df = reduce(lambda left, right: pd.merge(left, right, on = ['date'], how = \"outer\"), [data_monthly,gdp_df])\n",
    "    #full_df = full_df[['date','GDP_Gap']+indicator_ls]\n",
    "    new_earliest_date = list(data_monthly['date'])[0]\n",
    "    full_df = full_df[full_df['date'] >= new_earliest_date]\n",
    "    return full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c443719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a given machine learning model to predict GDP growth gap monthly\n",
    "def generate_ml_predictions(df, calcs_monthly, calcs_quarterly, stat_dict, contributions_dict, date_dict, model):\n",
    "    #df, calcs_monthly, calcs_quarterly, stat_dict, contributions_dict = prep_data(added_data_dict, monthly_info, quarterly_info)\n",
    "    model_df = df.copy().dropna()\n",
    "    model_df = model_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    preds = []\n",
    "    if date_dict['start_date'] != '':\n",
    "        model_df = model_df[model_df['date'] >= date_dict['start_date']].reset_index().drop(columns=['index'])\n",
    "    if date_dict['end_date'] != '':\n",
    "        if date_dict['end_date'][-2:] != '01':\n",
    "            date_dict['end_date'] = date_dict['end_date'][:-2] +'01'\n",
    "        num_oos =  len(model_df) - model_df[model_df['date'] == date_dict['end_date']].index[0]\n",
    "    else:\n",
    "        test_size = 0.5\n",
    "        num_oos = round(len(model_df)*test_size)\n",
    "    actual_vals = list(model_df['GDP_Gap'])\n",
    "    window_preds = list(model_df['GDP_Gap'])\n",
    "    shap_dict = {}\n",
    "    count = 1\n",
    "    model_df = model_df.set_index('date')\n",
    "    for i in range(-num_oos, 0, 1):\n",
    "        data_train = model_df[:i]\n",
    "        data_test = model_df[i:]\n",
    "        x_train = data_train.drop(columns=['GDP_Gap'])\n",
    "        y_train = data_train[['GDP_Gap']]\n",
    "        x_test = data_test.drop(columns=['GDP_Gap'])\n",
    "        y_test = data_test[['GDP_Gap']]\n",
    "        model.fit(x_train, y_train)\n",
    "        try:\n",
    "            pred = model.predict(x_test[:1])[0][0]\n",
    "        except:\n",
    "            pred = model.predict(x_test[:1])[0]\n",
    "        if math.isnan(actual_vals[i]):\n",
    "            actual_vals[i] = pred\n",
    "            model_df['GDP_Gap'] = actual_vals\n",
    "        window_preds[i] = pred\n",
    "        explainer = shap.Explainer(model.predict, x_test[:1])\n",
    "        shap_values = explainer(x_test[:1])\n",
    "        shap_dict[count] = list(shap_values[0].values + shap_values[0].base_values/len(shap_values[0].values))\n",
    "        count+=1\n",
    "    model_df['EAI'] = window_preds\n",
    "    shap_df = pd.DataFrame(shap_dict).T\n",
    "    shap_df.columns = list(model_df.drop(columns = ['GDP_Gap','EAI']).columns)\n",
    "    shap_df['date'] = list(model_df[-num_oos:].reset_index()['date'])\n",
    "    combined_dict = {}\n",
    "    for key in contributions_dict.keys():\n",
    "        new_cols = []\n",
    "        for col in contributions_dict[key]:\n",
    "            new_cols.append(re.sub('[^A-Za-z0-9_]+', '', col))\n",
    "        try:\n",
    "            new_df = shap_df[new_cols]\n",
    "        except:\n",
    "            new_cols_ls = []\n",
    "            for c in new_cols:\n",
    "                if c in shap_df.columns:\n",
    "                    new_cols_ls.append(c)\n",
    "            new_df = shap_df[new_cols_ls]\n",
    "        new_df[key] = new_df[list(new_df.columns)].sum(axis=1)\n",
    "        combined_dict[key] = list(new_df[key])\n",
    "    shap_df_2 = pd.DataFrame(combined_dict)\n",
    "    shap_df_2['date'] = list(model_df[-num_oos:].reset_index()['date'])\n",
    "    full_df = reduce(lambda left, right: pd.merge(left, right, on = ['date'], how = \"outer\"), [model_df.reset_index(),shap_df_2])\n",
    "    data_monthly = reduce(lambda left, right: pd.merge(left, right, on = ['date'], how = \"outer\"), [calcs_monthly,full_df[['date','EAI','consumption','exo','government','trade','financial','investment']]])\n",
    "    return full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2b66f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights(overall, val):\n",
    "    try:\n",
    "        return val/overall\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08b6de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get initial components per economic bucket using their shap values and weights\n",
    "def get_eai_components(full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly):\n",
    "    eai_components = data_monthly[['date','EAI','consumption','exo','government','trade','financial','investment']]\n",
    "    rmse_df = data_monthly[['date','EAI','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']]\n",
    "    ovr_weights = {}\n",
    "    for col in ['consumption','exo','government','trade','financial','investment']:\n",
    "        data_monthly[col+'_weight'] = data_monthly.apply(lambda x: find_weights(x['EAI'],x[col]),axis=1)\n",
    "        ovr_weights[col] = data_monthly[col+'_weight'].mean()\n",
    "    return eai_components, rmse_df, ovr_weights, calcs_quarterly, stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98541ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the entire machine learning workflow\n",
    "def ML_workflow(added_data_dict, date_dict, monthly_info, quarterly_info, model):\n",
    "    print('data prep')\n",
    "    full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict = prep_data(added_data_dict, monthly_info, quarterly_info)\n",
    "    print('ML preds')\n",
    "    full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly = generate_ml_predictions(full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict, date_dict, model)\n",
    "    print('components calculations')\n",
    "    eai_components, rmse_df, ovr_weights, calcs_quarterly, stat_dict = get_eai_components(full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly)\n",
    "    print('done')\n",
    "    return eai_components, rmse_df, ovr_weights, calcs_quarterly, stat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3f612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Components and Summary Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e770071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate summary statistics for quarterly predictions\n",
    "def calculate_quarterly_summaries(df):\n",
    "    df['quarter'] = df['date'].dt.to_period('Q')\n",
    "    quarterly_df = df.groupby('quarter', as_index=False, sort=False).mean()\n",
    "    summary_dict = {}\n",
    "    for col in ['HPBasedCycle','GDP_Gap','EAI']:\n",
    "        summary_dict[col] = {}\n",
    "        summary_dict[col]['mean'] = quarterly_df[col].mean()\n",
    "        summary_dict[col]['stdev'] = quarterly_df[col].std()\n",
    "    return quarterly_df, summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ee43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate overall components for EAI, GDP gap and HP based cycle metrics\n",
    "def calculate_overall_components(df_monthly, summary_dict, ovr_weights_dict):\n",
    "    eai_ls = list(df_monthly['EAI'])\n",
    "    comps_dict = {'consumption': list(df_monthly['consumption']), 'exo': list(df_monthly['exo']), 'financial': list(df_monthly['financial']), \n",
    "                  'government': list(df_monthly['government']), 'investment': list(df_monthly['investment']), 'trade': list(df_monthly['trade'])}\n",
    "    for comp in ['GDP_Gap','HPBasedCycle']:\n",
    "        new_eai_ls = []\n",
    "        for i in range(len(eai_ls)):\n",
    "            new_eai_ls.append(summary_dict[comp]['mean']+(eai_ls[i]-summary_dict['EAI']['mean'])*(summary_dict[comp]['stdev']/summary_dict['EAI']['stdev']))\n",
    "        df_monthly['EAI_'+comp] = new_eai_ls\n",
    "        for key in comps_dict.keys():\n",
    "            comp_ls = comps_dict[key]\n",
    "            new_comp_ls = []\n",
    "            for i in range(len(comp_ls)):\n",
    "                new_comp_ls.append(new_eai_ls[i]*ovr_weights_dict[key])\n",
    "            df_monthly[key+'_'+comp] = new_comp_ls\n",
    "    return df_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17989055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all previous functions to generate the overall components dataframe\n",
    "def get_overall_components(added_data_dict, date_dict, monthly_info, quarterly_info, model, verbose = False):\n",
    "    if verbose == True:\n",
    "        print('Extracting data and calculating EAI values for ' + str(country) + '...')\n",
    "    eai_comps, rmse_df, ovr_weights, calcs_quarterly, cycle_stat_dict = ML_workflow(added_data_dict, date_dict, monthly_info, quarterly_info, model)\n",
    "    if verbose == True:\n",
    "        print('Calculating quarterly summaries of real data for ' + str(country) + '...')\n",
    "    new_rmse_df, summary_dict = calculate_quarterly_summaries(rmse_df)\n",
    "    if verbose == True:\n",
    "        print('Calculating overall component figures for ' + str(country) + '...')\n",
    "    overall_eai_components = calculate_overall_components(eai_comps, summary_dict, ovr_weights)\n",
    "    quarterly_data = calcs_quarterly[['quarter','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']]\n",
    "    return overall_eai_components, quarterly_data, cycle_stat_dict, summary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34789ccb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1734579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match the lengths of a dictionary using null values to prepare for dataframe conversion\n",
    "def match_dict_lens(dict_to_match):\n",
    "    ls_lens = []\n",
    "    for key in dict_to_match.keys():\n",
    "        ls_lens.append(len(dict_to_match[key]))\n",
    "    max_len = max(ls_lens)\n",
    "    for key in dict_to_match.keys():\n",
    "        if len(dict_to_match[key]) < max_len:\n",
    "            while len(dict_to_match[key]) < max_len:\n",
    "                dict_to_match[key].append(np.nan)\n",
    "    return dict_to_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55902fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the next x months using a linear forecasting method (ARIMA)\n",
    "def predict_EAI(df, months_to_predict):\n",
    "    df = df.reset_index()[['date','EAI']]\n",
    "    mod = ARIMA((df['EAI'].astype(float)), order=(1,0,0))\n",
    "    res = mod.fit()\n",
    "    max_rows = len(df)-1\n",
    "    preds = list(res.predict(max_rows, max_rows + months_to_predict))[-months_to_predict:]\n",
    "    ci95 = res.get_forecast(months_to_predict).conf_int(alpha=0.05)[:months_to_predict]\n",
    "    ci99 = res.get_forecast(months_to_predict).conf_int(alpha=0.01)[:months_to_predict]\n",
    "    eai_ls = list(df['EAI'])\n",
    "    ci95_lower_ls = eai_ls + list(ci95['lower EAI'])\n",
    "    ci95_upper_ls = eai_ls + list(ci95['upper EAI'])\n",
    "    ci99_lower_ls = eai_ls + list(ci99['lower EAI'])\n",
    "    ci99_upper_ls = eai_ls + list(ci99['upper EAI'])\n",
    "    pred_ls = eai_ls + preds\n",
    "    date_ls = list(df['date'])\n",
    "    end_date = date_ls[-1]  + np.timedelta64(months_to_predict+1, 'M')\n",
    "    date_range = list(pd.date_range(date_ls[0], end_date, freq='MS'))\n",
    "    new_df_dict = {'date': date_range, 'EAI': eai_ls, 'pred': pred_ls, \n",
    "                   '95%CILowerLimit': ci95_lower_ls, '95%CIUpperLimit': ci95_upper_ls,\n",
    "                   '99%CILowerLimit': ci99_lower_ls, '99%CIUpperLimit': ci99_upper_ls}\n",
    "    new_df_dict = match_dict_lens(new_df_dict)\n",
    "    return pd.DataFrame(new_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04aaa218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run previous functions to get the components, statistics, and predictions\n",
    "def get_EAI_update(added_data_dict, date_dict, monthly_info, quarterly_info, model, months_to_predict = 3, verbose = False):\n",
    "    eai_components, real_quarterly_data, cycle_stats, summary_dict = get_overall_components(added_data_dict, date_dict, monthly_info, quarterly_info, model, verbose=verbose)\n",
    "    try:\n",
    "        eai_components['date'] = pd.to_datetime(eai_components['date'])\n",
    "        eai_components = eai_components.sort_values(by=['date'], ascending = True)\n",
    "    except:\n",
    "        pass\n",
    "    new_components = eai_components.copy()\n",
    "    if verbose == True:\n",
    "        print('Generating EAI predictions and confidence intervals for the next ' + str(months_to_predict) + ' months for ' + str(country) + '...')\n",
    "    eai_preds = predict_EAI(new_components, months_to_predict)\n",
    "    real_quarterly_data = real_quarterly_data.set_index('quarter').dropna(axis = 0, how = 'all').reset_index()\n",
    "    if verbose == True:\n",
    "        print('Finished generating figures for ' + str(country) + '.')\n",
    "    return eai_components, real_quarterly_data, cycle_stats, eai_preds, summary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228540c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Additional Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4113b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a dataframe into an excel file and sheet\n",
    "def write_excel(filename, sheet_name, df):\n",
    "    if not os.path.isfile(filename):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        wb.save(filename)\n",
    "        wb.close()\n",
    "    og_cols = list(df.columns)\n",
    "    cols=pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique(): \n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns=cols\n",
    "    wb = load_workbook(filename)\n",
    "    if sheet_name in wb.get_sheet_names():\n",
    "        sheet_to_del=wb.get_sheet_by_name(sheet_name)\n",
    "        wb.remove_sheet(sheet_to_del)\n",
    "    wb.create_sheet(sheet_name)\n",
    "    ws = wb[sheet_name]\n",
    "    w_exc_dict = {}\n",
    "    cols = list(df.columns)\n",
    "    for c in cols:\n",
    "        w_exc_dict[c] = list(df[c])\n",
    "    col_count = 1\n",
    "    for d in w_exc_dict.keys():\n",
    "        ws.cell(row=1, column=col_count).value = og_cols[col_count-1]\n",
    "        for i in range(len(w_exc_dict[d])):\n",
    "            try:\n",
    "                w_exc_dict[d][i] = w_exc_dict[d][i].replace(tzinfo=None)\n",
    "            except:\n",
    "                pass\n",
    "            ws.cell(row=i+2, column=col_count).value = w_exc_dict[d][i]\n",
    "        try:\n",
    "            m_row = ws.max_row\n",
    "            for i in range(m_row+1):\n",
    "                try:\n",
    "                    ws.cell(row=i+1, column=col_count).value = ws.cell(row=i+1, column=col_count).value.strftime(\"%m/%d/%Y\")\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "        col_count+=1\n",
    "    wb.save(filename)\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "656b6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the average cycles, eai components, eai predictions, quarterly eai and quarterly gdp gap sheets\n",
    "def prepare_country_file(components, qtr_data, stats, preds, summ_dict, filename):\n",
    "    #Average Cycle\n",
    "    metrics = ['Total No. of TPs','Ave. duration of cycles (qtr)','Ave. duration of slowdown (qtr)','Ave. duration of expansion (qtr)','Ave. amplitude of slowdown','Ave. amplitude of expansion','Speed of slowdown','Speed of expansion','CV_d of slowdown','CV_d of expansion','CV_a of slowdown','CV_a of expansion','Ave. RGDP growth of slowdown','Ave. RGDP growth of expansion','Ave. credit/gdp growth of slowdown','Ave. credit/gdp growth of expansion','Number of quarters from last peak']\n",
    "    values = list(stats['GDP_Gap_norm'].values())[1:] + [np.nan,np.nan,np.nan,np.nan,stats['summ_stat']['No. of Quarters from Last Peak']]\n",
    "    avg_cycles_df = pd.DataFrame({'Metric': metrics, 'Value': values})\n",
    "    \n",
    "    #EAI_Components\n",
    "    comps = components.copy()\n",
    "    comps.columns = ['month','EAI','CONSUMPTION','EXO_INTERNATIONAL','GOVERNMENT','TRADE','FINANCIAL','INVESTMENT',\n",
    "                          'EAI_GDP_gap','CONSUMPTION_GDP_gap','EXO_INTERNATIONAL_GDP_gap','FINANCIAL_GDP_gap','GOVERNMENT_GDP_gap','INVESTMENT_GDP_gap','TRADE_GDP_gap',\n",
    "                          'EAI_HP','CONSUMPTION_HP','EXO_INTERNATIONAL_HP','FINANCIAL_HP','GOVERNMENT_HP','INVESTMENT_HP','TRADE_HP']\n",
    "    comps = comps[['month','EAI','CONSUMPTION','EXO_INTERNATIONAL','FINANCIAL','GOVERNMENT','INVESTMENT','TRADE',\n",
    "                          'EAI_GDP_gap','CONSUMPTION_GDP_gap','EXO_INTERNATIONAL_GDP_gap','FINANCIAL_GDP_gap','GOVERNMENT_GDP_gap','INVESTMENT_GDP_gap','TRADE_GDP_gap',\n",
    "                          'EAI_HP','CONSUMPTION_HP','EXO_INTERNATIONAL_HP','FINANCIAL_HP','GOVERNMENT_HP','INVESTMENT_HP','TRADE_HP']]\n",
    "    comps_dict = {}\n",
    "    for col in list(comps.columns):\n",
    "        comps_dict[col] = list(comps[col])\n",
    "\n",
    "    comps_dict['blank1'] = [np.nan]\n",
    "    comps_dict['blank2'] = [np.nan]\n",
    "    comps_dict['Variable'] = ['GDP_Gap','HPBasedCycle','WALS']\n",
    "    comps_dict['Mean'] = [summ_dict['GDP_Gap']['mean'],summ_dict['HPBasedCycle']['mean'],summ_dict['EAI']['mean']]\n",
    "    comps_dict['Std. Dev.'] = [summ_dict['GDP_Gap']['stdev'],summ_dict['HPBasedCycle']['stdev'],summ_dict['EAI']['stdev']]\n",
    "    comps_dict = match_dict_lens(comps_dict)\n",
    "    eai_components_df = pd.DataFrame(comps_dict)\n",
    "\n",
    "    \n",
    "    #if column contains only 0, then change all values to NaN\n",
    "    for col in list(eai_components_df.columns):\n",
    "        if (eai_components_df[col].dropna() == 0).all():\n",
    "            eai_components_df[col] = np.nan\n",
    "\n",
    "    #EAI_prediction\n",
    "    new_preds = preds.copy()\n",
    "    new_preds.columns = ['month','EAI','PredictedEAI','95%CiLowerLimit','95%CiUpperLimit','99%CiLowerLimit', '99%CiUpperLimit']\n",
    "    \n",
    "    #EAI_quarterly\n",
    "    eai_qtr = eai_components_df[['month','EAI']]\n",
    "    eai_qtr['qtr'] = eai_qtr['month'].dt.to_period('Q')\n",
    "    eai_qtr = eai_qtr.groupby('qtr', as_index=False, sort=False).mean()\n",
    "    eai_qtr = eai_qtr.drop(columns=['month'])\n",
    "    eai_qtr = eai_qtr[['qtr','EAI']]\n",
    "    eai_qtr['qtr'] = pd.PeriodIndex(eai_qtr['qtr'], freq='Q').to_timestamp()\n",
    "    \n",
    "    #GDP_gap_quarterly\n",
    "    q_df = qtr_data.copy()\n",
    "    q_df['qtr'] = pd.PeriodIndex(q_df['quarter'], freq='Q').to_timestamp()\n",
    "    q_df = q_df[q_df['qtr'] >= datetime(2000,1,1)]\n",
    "    q_df_gdp = q_df[['qtr','GDP_Gap','HPBasedCycle']]\n",
    "    \n",
    "    #filename = \"C:/Users/Patrick Jaime Simba/Desktop/ADB Files/Notebooks/ML Time Series/ML Methodology/Updated Files V2/EAI_\" + country + \".xlsx\"\n",
    "    write_excel(filename, 'Average Cycles', avg_cycles_df)\n",
    "    write_excel(filename, 'EAI_Components', eai_components_df)\n",
    "    write_excel(filename, 'EAI_prediction', new_preds)\n",
    "    write_excel(filename, 'EAI_quarterly', eai_qtr)\n",
    "    write_excel(filename, 'GDP_gap_quarterly', q_df_gdp)\n",
    "    \n",
    "    return avg_cycles_df, eai_components_df, new_preds, eai_qtr, q_df_gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e02553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use previous functions to prepare all relevant data and save files\n",
    "def generate_EAI_update(added_data_dict, date_dict, monthly_info, quarterly_info, model, filename, verbose = False):\n",
    "    components, qtr_data, stats, preds, summary_dict = get_EAI_update(added_data_dict, date_dict, monthly_info, quarterly_info, model)\n",
    "    if verbose:\n",
    "        print(\"Preparing country EAI excel file...\")\n",
    "    avg_cycles, eai_comps, eai_preds, eai_qtrly, gdp_hp_qtrly = prepare_country_file(components, qtr_data, stats, preds, summary_dict, filename)\n",
    "    if verbose:\n",
    "        print(\"EAI_\"+country+\".xlsx file successfully generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d494e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Additional Excel Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a4ce7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open an excel file and particular sheet\n",
    "def open_sheet(filename, sheet):\n",
    "    df = pd.read_excel(filename, sheet_name=sheet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94806216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the coordinates for each predicted value and GDP gap for the business cycle dials\n",
    "def find_eai_dial_coordinates(curr, prev, axis, trend):\n",
    "    try:\n",
    "        if axis == 'x':\n",
    "            if (curr >= prev) and (curr >= trend):\n",
    "                return 1.7\n",
    "            elif (curr >= prev) and (curr < trend):\n",
    "                return 0.3\n",
    "            elif (curr < prev) and (curr >= trend):\n",
    "                return 2.5\n",
    "            elif (curr < prev) and (curr < trend):\n",
    "                return -0.5\n",
    "            else:\n",
    "                return ''\n",
    "        elif axis == 'y':\n",
    "            if (curr >= prev) and (curr >= trend):\n",
    "                return 2.25\n",
    "            elif (curr >= prev) and (curr < trend):\n",
    "                return 2.25\n",
    "            elif (curr < prev) and (curr >= trend):\n",
    "                return 1.35\n",
    "            elif (curr < prev) and (curr < trend):\n",
    "                return 1.35\n",
    "            else:\n",
    "                return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e40a6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the coordinates for the sector component dials\n",
    "def find_component_dial_coordinates(curr, axis):\n",
    "    try:\n",
    "        if axis == 'x':\n",
    "            if (curr > 0):\n",
    "                return 2.2\n",
    "            elif (curr < 0):\n",
    "                return -0.2\n",
    "            elif (curr == 0):\n",
    "                return 1\n",
    "            else:\n",
    "                return ''\n",
    "        elif axis == 'y':\n",
    "            if (curr > 0):\n",
    "                return 2.05\n",
    "            elif (curr < 0):\n",
    "                return 2.05\n",
    "            elif (curr == 0):\n",
    "                return 2.5\n",
    "            else:\n",
    "                return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "375ad6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and create excel sheets for the final file\n",
    "def create_dial_coordinates_sheet(filename, trend=0):\n",
    "    components = open_sheet(filename, 'EAI_Components')\n",
    "    #components = components.dropna(subset=['EAI'])\n",
    "    comps_dict = {'eai': list(components['EAI']), 'con': list(components['CONSUMPTION']), \n",
    "                 'exo': list(components['EXO_INTERNATIONAL']), 'fin': list(components['FINANCIAL']),\n",
    "                 'gov': list(components['GOVERNMENT']), 'inv': list(components['INVESTMENT']), \n",
    "                 'tra': list(components['TRADE'])}\n",
    "    new_data_dict = {'month': list(components['month'])}\n",
    "    for key in comps_dict.keys():\n",
    "        new_data_dict[key] = comps_dict[key]\n",
    "        prev_ls = ['']\n",
    "        for i in range(len(comps_dict[key])-1):\n",
    "            prev_ls.append(comps_dict[key][i])\n",
    "        new_data_dict['prev_'+key] = prev_ls\n",
    "        x_ls = []\n",
    "        y_ls = []\n",
    "        for i in range(len(new_data_dict[key])):\n",
    "            if key == 'eai':\n",
    "                x_ls.append(find_eai_dial_coordinates(new_data_dict[key][i], new_data_dict['prev_'+key][i], 'x', trend))\n",
    "                y_ls.append(find_eai_dial_coordinates(new_data_dict[key][i], new_data_dict['prev_'+key][i], 'y', trend))\n",
    "            else:\n",
    "                x_ls.append(find_component_dial_coordinates(new_data_dict[key][i], 'x'))\n",
    "                y_ls.append(find_component_dial_coordinates(new_data_dict[key][i], 'y'))\n",
    "        new_data_dict['x_param_'+key] = x_ls\n",
    "        new_data_dict['y_param_'+key] = y_ls\n",
    "    return pd.DataFrame(new_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8a86983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_vals_sheet(filename):\n",
    "    predictions = open_sheet(filename, 'EAI_prediction')\n",
    "    gdp_gap = open_sheet(filename, 'GDP_gap_quarterly')\n",
    "    gdp_gap = gdp_gap[['qtr','GDP_Gap']]\n",
    "    gdp_gap.columns = ['month','GDP_Gap']\n",
    "    gdp_gap['month'] = pd.to_datetime(gdp_gap['month'])\n",
    "    pred_dict = {'month': list(predictions['month']), 'EAI': list(predictions['EAI']), \n",
    "                 'PredictedEAI': list(predictions['EAI'])[:-3]+list(predictions['PredictedEAI'])[-3:]}\n",
    "    for i in range(len(pred_dict['month'])):\n",
    "        if type(pred_dict['month'][i]) == int:\n",
    "            pred_dict['month'][i] = datetime.fromordinal(datetime(1900, 1, 1).toordinal() + pred_dict['month'][i] - 2)\n",
    "    \n",
    "    ucl95 = []\n",
    "    lcl95 = []\n",
    "    ucl99 = []\n",
    "    lcl99 = []\n",
    "    for i in range(len(pred_dict['EAI'][:-4])):\n",
    "        ucl95.append(np.nan)\n",
    "        lcl95.append(np.nan)\n",
    "        ucl99.append(np.nan)\n",
    "        lcl99.append(np.nan)\n",
    "    ucl95.append(list(predictions['EAI'])[-4])\n",
    "    lcl95.append(list(predictions['EAI'])[-4])\n",
    "    ucl99.append(list(predictions['EAI'])[-4])\n",
    "    lcl99.append(list(predictions['EAI'])[-4])\n",
    "\n",
    "    for j in [-3,-2,-1]:\n",
    "        ucl95.append(list(predictions['95%CiUpperLimit'])[j])\n",
    "        lcl95.append(list(predictions['95%CiLowerLimit'])[j])\n",
    "        ucl99.append(list(predictions['99%CiUpperLimit'])[j])\n",
    "        lcl99.append(list(predictions['99%CiLowerLimit'])[j])\n",
    "\n",
    "    \n",
    "    pred_dict['UCL95'] = ucl95\n",
    "    pred_dict['LCL95'] = lcl95\n",
    "    pred_dict['UCL99'] = ucl99\n",
    "    pred_dict['LCL99'] = lcl99\n",
    "    \n",
    "    new_df = pd.DataFrame(pred_dict)\n",
    "    new_df['month'] = pd.to_datetime(new_df['month'])\n",
    "    final_df = reduce(lambda left, right: pd.merge(left, right, on = ['month'], how = \"outer\"), [new_df, gdp_gap])\n",
    "    #final_df['month'] = pd.to_datetime(final_df['month'])\n",
    "    #final_df = final_df.sort_values(by=['month'], ascending = True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "091c612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quarterly_computations_sheet(filename):\n",
    "    eai_df = open_sheet(filename, 'EAI_Components')\n",
    "    eai_df_2 = open_sheet(filename, 'EAI_quarterly')\n",
    "    gdp_df = open_sheet(filename, 'GDP_gap_quarterly')\n",
    "    full_df_1 = reduce(lambda left, right: pd.merge(left , right,on = [\"qtr\"],how = \"outer\"),[eai_df_2,gdp_df])\n",
    "    d_dict = {'month': list(eai_df['month']), 'EAI': list(eai_df['EAI']), 'quarter': list(full_df_1['qtr']), \n",
    "              'EAI_quarterly': list(full_df_1['EAI']), 'GDP_gap_quarterly': list(full_df_1['GDP_Gap']),\n",
    "              'HPBasedCycle_quarterly': list(full_df_1['HPBasedCycle'])}\n",
    "    d_dict = match_dict_lens(d_dict)\n",
    "    return pd.DataFrame(d_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdff8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quarterly_data_sheet(filename, trend = 0):\n",
    "    quarterly_comps = open_sheet(filename, 'Quarterly_computations')\n",
    "    eai_quarterly = open_sheet(filename, 'EAI_quarterly')\n",
    "    qtr_dict = {}\n",
    "    qtr_dict['quarter'] = list(eai_quarterly['qtr'])\n",
    "    \n",
    "    eai_ls = list(quarterly_comps['EAI_quarterly'])\n",
    "    #eai_ls = [item for item in eai_ls if not(math.isnan(item))==True]\n",
    "    qtr_dict['EAI_quarterly'] = eai_ls\n",
    "    prev_eai_ls = ['']\n",
    "    for i in eai_ls[:-1]:\n",
    "        prev_eai_ls.append(i)\n",
    "    qtr_dict['prev_eai_quarterly'] = prev_eai_ls\n",
    "    qtr_dict['x_param_eai_quarterly'] = []\n",
    "    qtr_dict['y_param_eai_quarterly'] = []\n",
    "    for i in range(len(qtr_dict['EAI_quarterly'])):\n",
    "        qtr_dict['x_param_eai_quarterly'].append(find_eai_dial_coordinates(qtr_dict['EAI_quarterly'][i], qtr_dict['prev_eai_quarterly'][i], 'x', trend))\n",
    "        qtr_dict['y_param_eai_quarterly'].append(find_eai_dial_coordinates(qtr_dict['EAI_quarterly'][i], qtr_dict['prev_eai_quarterly'][i], 'y', trend))\n",
    "        \n",
    "    gdp_ls = list(quarterly_comps['GDP_gap_quarterly'])\n",
    "    #gdp_ls = [item for item in gdp_ls if not(math.isnan(item))==True]\n",
    "    while math.isnan(gdp_ls[-1]):\n",
    "            del gdp_ls[-1]\n",
    "    qtr_dict['GDP_gap_quarterly'] = gdp_ls\n",
    "    prev_gdp_ls = ['']\n",
    "    for i in gdp_ls[:-1]:\n",
    "        prev_gdp_ls.append(i)\n",
    "    qtr_dict['prev_gdp_gap_quarterly'] = prev_gdp_ls\n",
    "    qtr_dict['x_param_gdp_gap_quarterly'] = []\n",
    "    qtr_dict['y_param_gdp_gap_quarterly'] = []\n",
    "    for i in range(len(qtr_dict['GDP_gap_quarterly'])):\n",
    "        qtr_dict['x_param_gdp_gap_quarterly'].append(find_eai_dial_coordinates(qtr_dict['GDP_gap_quarterly'][i], qtr_dict['prev_gdp_gap_quarterly'][i], 'x', trend))\n",
    "        qtr_dict['y_param_gdp_gap_quarterly'].append(find_eai_dial_coordinates(qtr_dict['GDP_gap_quarterly'][i], qtr_dict['prev_gdp_gap_quarterly'][i], 'y', trend))\n",
    "    \n",
    "    try:\n",
    "        hp_ls = list(quarterly_comps['HPBasedCycle_quarterly'])\n",
    "        #hp_ls = [item for item in hp_ls if not(math.isnan(item))==True]\n",
    "        while math.isnan(hp_ls[-1]):\n",
    "            del hp_ls[-1]\n",
    "        qtr_dict['HPBasedCycle_quarterly'] = hp_ls\n",
    "        prev_hp_ls = ['']\n",
    "        for i in hp_ls[:-1]:\n",
    "            prev_hp_ls.append(i)\n",
    "        qtr_dict['prev_HPBasedCycle_quarterly'] = prev_hp_ls\n",
    "        qtr_dict['x_param_HPBasedCycle_quarterly'] = []\n",
    "        qtr_dict['y_param_HPBasedCycle_quarterly'] = []\n",
    "        for i in range(len(qtr_dict['HPBasedCycle_quarterly'])):\n",
    "            qtr_dict['x_param_HPBasedCycle_quarterly'].append(find_eai_dial_coordinates(qtr_dict['HPBasedCycle_quarterly'][i], qtr_dict['prev_HPBasedCycle_quarterly'][i], 'x', trend))\n",
    "            qtr_dict['y_param_HPBasedCycle_quarterly'].append(find_eai_dial_coordinates(qtr_dict['HPBasedCycle_quarterly'][i], qtr_dict['prev_HPBasedCycle_quarterly'][i], 'y', trend))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for key in qtr_dict.keys():\n",
    "        #print(key)\n",
    "        while pd.isnull(qtr_dict[key][-1]):\n",
    "            del qtr_dict[key][-1]\n",
    "    \n",
    "    qtr_dict = match_dict_lens(qtr_dict)\n",
    "    return pd.DataFrame(qtr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35dd941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment1_formula(val, amp, x_intercept, shift):\n",
    "    b_val = x_intercept/math.acos(0)\n",
    "    return amp * math.cos(val/b_val)\n",
    "\n",
    "def segment2_formula(val, amp, x_intercept, shift):\n",
    "    b_val = x_intercept/math.acos(0)\n",
    "    return -amp * math.cos(val/b_val)\n",
    "\n",
    "def segment3_formula(val, amp, x_intercept, shift):\n",
    "    b_val = x_intercept/math.acos(0)\n",
    "    return amp * math.cos((val-shift)/b_val)\n",
    "\n",
    "def segment4_formula(val, amp, x_intercept, shift):\n",
    "    b_val = x_intercept/math.acos(0)\n",
    "    return -amp * math.cos((val-shift)/b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be3d0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_quarter_constant(latest, prev, middle, maximum):\n",
    "    if latest > prev and latest > 0:\n",
    "        return maximum-((maximum-middle)/4)\n",
    "    elif latest > prev and latest < 0:\n",
    "        return middle+((maximum-middle)/4)\n",
    "    elif latest < prev and latest < 0:\n",
    "        return middle-((maximum-middle)/4)\n",
    "    elif latest < prev and latest > 0:\n",
    "        return ((maximum-middle)/4)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cea87ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cycle_position_v0(curr, prev, trend):\n",
    "    if curr > trend and curr >= prev:\n",
    "        return 12.8\n",
    "    elif curr > trend and curr < prev:\n",
    "        return 2.2\n",
    "    elif curr < trend and curr >= prev:\n",
    "        return 9.0\n",
    "    else:\n",
    "        return 5.9\n",
    "    \n",
    "def check_cycle_position(curr, prev, trend):\n",
    "    if curr > trend and curr >= prev:\n",
    "        return 9.0\n",
    "    elif curr > trend and curr < prev:\n",
    "        return 12.8\n",
    "    elif curr < trend and curr >= prev:\n",
    "        return 5.9\n",
    "    else:\n",
    "        return 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1b4d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_curved_data_sheet(filename, trend = 0):\n",
    "    quarterly_data_df = open_sheet(filename, 'Quarterly_data')\n",
    "    avg_cycles = open_sheet(filename, 'Average Cycles')\n",
    "    cycles_dict = {}\n",
    "    for i in range(len(list(avg_cycles['Metric']))):\n",
    "        cycles_dict[list(avg_cycles['Metric'])[i]] = list(avg_cycles['Value'])[i]\n",
    "    latest_quarterly_eai = [x for x in list(quarterly_data_df['EAI_quarterly']) if str(x) != 'nan'][-1]\n",
    "    previous_quarterly_eai = [x for x in list(quarterly_data_df['EAI_quarterly']) if str(x) != 'nan'][-2]\n",
    "    latest_quarter_val_ls = [x for x in list(quarterly_data_df['quarter']) if str(x) != 'nan']\n",
    "    gdp_gap_quarterly_ls = [x for x in list(quarterly_data_df['GDP_gap_quarterly']) if str(x) != 'nan']\n",
    "    latest_quarterly_gdp = gdp_gap_quarterly_ls[-1]\n",
    "    previous_quarterly_gdp = gdp_gap_quarterly_ls[-2]\n",
    "    curved_dict = {'x_initial': [], 'y_initial': [], 'x': [], 'y': [], 'latest_y_val': [],\n",
    "                 'latest_quarterly_month': [latest_quarter_val_ls[-1]], 'latest_quarterly_month_gdp': [latest_quarter_val_ls[-2]], 'latest_quarterly_eai': [latest_quarterly_eai], \n",
    "                 'previous_quarterly_eai': [previous_quarterly_eai], 'latest_quarterly_gdp': [latest_quarterly_gdp], \n",
    "                 'previous_quarterly_gdp': [previous_quarterly_gdp], 'latest_quarterly_quarter': [ round(find_latest_quarter_constant(latest_quarterly_eai, previous_quarterly_eai, cycles_dict['Ave. duration of slowdown (qtr)'], cycles_dict['Ave. duration of cycles (qtr)']),1)]}\n",
    "    curved_dict['x_initial'] = [0, cycles_dict['Ave. duration of slowdown (qtr)'], cycles_dict['Ave. duration of cycles (qtr)']]\n",
    "    curved_dict['y_initial'] = [cycles_dict['Ave. amplitude of expansion'], -cycles_dict['Ave. amplitude of slowdown'], cycles_dict['Ave. amplitude of expansion']]\n",
    "    x_ls = list(np.round(np.arange(0.0, round(15,1)+0.1, 0.1),1))\n",
    "    y_ls = []\n",
    "    for i in x_ls:\n",
    "        y_ls.append(3*math.cos(0.5*(i) - 5.313))\n",
    "    curved_dict['x'] = x_ls\n",
    "    curved_dict['y'] = y_ls\n",
    "    cycle_pos = check_cycle_position(curved_dict['latest_quarterly_eai'][0], curved_dict['previous_quarterly_eai'][0], trend)\n",
    "    #print(cycle_pos)\n",
    "    latest_y_val_ls = []\n",
    "    for i in range(len(x_ls)):\n",
    "        if x_ls[i] == cycle_pos:\n",
    "            latest_y_val_ls.append(y_ls[i])\n",
    "        else:\n",
    "            latest_y_val_ls.append(np.nan)\n",
    "    curved_dict['latest_y_val'] = latest_y_val_ls\n",
    "    latest_y_val_ls_gdp = []\n",
    "    cycle_pos_gdp = check_cycle_position(curved_dict['latest_quarterly_gdp'][0], curved_dict['previous_quarterly_gdp'][0], trend)\n",
    "    for i in range(len(x_ls)):\n",
    "        if x_ls[i] == cycle_pos_gdp:\n",
    "            latest_y_val_ls_gdp.append(y_ls[i])\n",
    "        else:\n",
    "            latest_y_val_ls_gdp.append(np.nan)\n",
    "    curved_dict['latest_y_val_gdp'] = latest_y_val_ls_gdp\n",
    "    curved_dict = match_dict_lens(curved_dict)\n",
    "    curved_df = pd.DataFrame(curved_dict)\n",
    "    return curved_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a0f7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gdp_curved_data_sheet(filename, trend = 0):\n",
    "    quarterly_data_df = open_sheet(filename, 'Quarterly_data')\n",
    "    avg_cycles = open_sheet(filename, 'Average Cycles')\n",
    "    cycles_dict = {}\n",
    "    for i in range(len(list(avg_cycles['Metric']))):\n",
    "        cycles_dict[list(avg_cycles['Metric'])[i]] = list(avg_cycles['Value'])[i]\n",
    "    gdp_gap_quarterly_ls = [x for x in list(quarterly_data_df['GDP_gap_quarterly']) if str(x) != 'nan']\n",
    "    latest_quarterly_eai = gdp_gap_quarterly_ls[-1]\n",
    "    previous_quarterly_eai = gdp_gap_quarterly_ls[-2]\n",
    "    latest_quarter_val_ls = [x for x in list(quarterly_data_df['quarter']) if str(x) != 'nan']\n",
    "    curved_dict = {'x_initial': [], 'y_initial': [], 'x': [], 'y': [], 'latest_y_val': [],\n",
    "                 'latest_quarterly_month': [latest_quarter_val_ls[-1]], 'latest_quarterly_gdp': [latest_quarterly_eai], \n",
    "                 'previous_quarterly_gdp': [previous_quarterly_eai], 'latest_quarterly_quarter': [ round(find_latest_quarter_constant(latest_quarterly_eai, previous_quarterly_eai, cycles_dict['Ave. duration of slowdown (qtr)'], cycles_dict['Ave. duration of cycles (qtr)']),1)]}\n",
    "    curved_dict['x_initial'] = [0, cycles_dict['Ave. duration of slowdown (qtr)'], cycles_dict['Ave. duration of cycles (qtr)']]\n",
    "    curved_dict['y_initial'] = [cycles_dict['Ave. amplitude of expansion'], -cycles_dict['Ave. amplitude of slowdown'], cycles_dict['Ave. amplitude of expansion']]\n",
    "    x_ls = list(np.round(np.arange(0.0, round(15,1)+0.1, 0.1),1))\n",
    "    y_ls = []\n",
    "    for i in x_ls:\n",
    "        y_ls.append(3*math.cos(0.5*(i) - 5.313))\n",
    "    curved_dict['x'] = x_ls\n",
    "    curved_dict['y'] = y_ls\n",
    "    cycle_pos = check_cycle_position(curved_dict['latest_quarterly_gdp'][0], curved_dict['previous_quarterly_gdp'][0], trend)\n",
    "    latest_y_val_ls = []\n",
    "    #print(curved_dict)\n",
    "    for i in range(len(x_ls)):\n",
    "        if x_ls[i] == cycle_pos:\n",
    "            latest_y_val_ls.append(y_ls[i])\n",
    "        else:\n",
    "            latest_y_val_ls.append(np.nan)\n",
    "    curved_dict['latest_y_val'] = latest_y_val_ls\n",
    "    curved_dict = match_dict_lens(curved_dict)\n",
    "    curved_df = pd.DataFrame(curved_dict)\n",
    "    return curved_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d51a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cycle_bands_sheet(filename):\n",
    "    cycle_data = open_sheet(filename, 'curved_data')\n",
    "    cycle_dict = {\n",
    "                 'blue_band_min': [list(cycle_data['x_initial'])[0]],\n",
    "                 'blue_band_max': [list(cycle_data['x_initial'])[1]/2],\n",
    "                 'red_band_min': [list(cycle_data['x_initial'])[1]/2],\n",
    "                 'red_band_max': [list(cycle_data['x_initial'])[1]],\n",
    "                 'yellow_band_min': [list(cycle_data['x_initial'])[1]],\n",
    "                 'yellow_band_max': [(list(cycle_data['x_initial'])[2]/4) + list(cycle_data['x_initial'])[1]],\n",
    "                 'green_band_min': [(list(cycle_data['x_initial'])[2]/4) + list(cycle_data['x_initial'])[1]],\n",
    "                 'green_band_max': [list(cycle_data['x_initial'])[2]]}\n",
    "    return pd.DataFrame(cycle_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32bc0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run previous functions to prepare additional calculation sheets for the dashboard\n",
    "def prepare_sheets(filepath):\n",
    "    \n",
    "    trend = 0\n",
    "    \n",
    "    dial_sheet = create_dial_coordinates_sheet(filepath, trend)\n",
    "    write_excel(filepath, 'Dial_Coordinates', dial_sheet)\n",
    "    \n",
    "    pred_sheet = create_pred_vals_sheet(filepath)\n",
    "    pred_sheet['gdp_trend'] = trend\n",
    "    write_excel(filepath, 'pred_vals', pred_sheet)\n",
    "    \n",
    "    qtr_comps_sheet = create_quarterly_computations_sheet(filepath)\n",
    "    write_excel(filepath, 'Quarterly_computations', qtr_comps_sheet)\n",
    "    \n",
    "    qtr_data_sheet = create_quarterly_data_sheet(filepath, trend)\n",
    "    write_excel(filepath, 'Quarterly_data', qtr_data_sheet)\n",
    "    \n",
    "    curved_sheet = create_curved_data_sheet(filepath, trend)\n",
    "    write_excel(filepath, 'curved_data', curved_sheet)\n",
    "    \n",
    "    gdp_curved_sheet = create_gdp_curved_data_sheet(filepath, trend)\n",
    "    write_excel(filepath, 'curved_gdp_data', curved_sheet)\n",
    "    \n",
    "    band_sheet = create_cycle_bands_sheet(filepath)\n",
    "    write_excel(filepath, 'cycle_bands', band_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cdd29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57910ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary containing all possible model types\n",
    "full_model_dictionary = {\n",
    "     'Linear Regression': LinearRegression(),\n",
    "     'Random Forest': RandomForestRegressor(),\n",
    "     'Light Gradient Boosting Method (LGBM)': LGBMRegressor(),\n",
    "     'Extreme Boosting (XGBoost)': XGBRegressor(),\n",
    "     'CatBoost': CatBoostRegressor(),\n",
    "     'Stochastic Gradient Descent (SGDR)': SGDRegressor(),\n",
    "     'Kernel Ridge': KernelRidge(),\n",
    "     'Elastic Net': ElasticNet(),\n",
    "     'Bayesian Ridge': BayesianRidge(),\n",
    "     'Gradient Boosting': GradientBoostingRegressor(),\n",
    "     'Support Vector Machine (SVM)': SVR()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ec19aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all back end operations\n",
    "def operations_func(added_data_dict, date_dict, monthly_info, quarterly_info, model_name, filepath, log_str):\n",
    "    \n",
    "    model = full_model_dictionary[model_name]\n",
    "    \n",
    "    log_str += 'Preparing data for ' + model_name +' model.\\n'\n",
    "    full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict = prep_data(added_data_dict, monthly_info, quarterly_info)\n",
    "    \n",
    "    log_str += 'Generating predictions.\\n'\n",
    "    full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly = generate_ml_predictions(full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict, date_dict, model)\n",
    "    \n",
    "    log_str += 'Calculating initial components.\\n'\n",
    "    eai_comps, rmse_df, ovr_weights, calcs_quarterly, cycle_stats = get_eai_components(full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly)\n",
    "    \n",
    "    log_str += 'Calculating data summaries.\\n'\n",
    "    new_rmse_df, summary_dict = calculate_quarterly_summaries(rmse_df)\n",
    "    \n",
    "    log_str += 'Calculating overall components.\\n'\n",
    "    eai_components = calculate_overall_components(eai_comps, summary_dict, ovr_weights)\n",
    "    real_quarterly_data = calcs_quarterly[['quarter','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']]\n",
    "    \n",
    "    try:\n",
    "        eai_components['date'] = pd.to_datetime(eai_components['date'])\n",
    "        eai_components = eai_components.sort_values(by=['date'], ascending = True)\n",
    "    except:\n",
    "        pass\n",
    "    new_components = eai_components.copy()\n",
    "    \n",
    "    log_str += 'Forecasting the next 3 months.\\n'\n",
    "    eai_preds = predict_EAI(new_components, 3)\n",
    "    real_quarterly_data = real_quarterly_data.set_index('quarter').dropna(axis = 0, how = 'all').reset_index()\n",
    "\n",
    "    log_str += 'Generating excel data.\\n'\n",
    "    filename = filepath +'/'+ model_name + '.xlsx'\n",
    "    avg_cycles, eai_comps, eai_preds, eai_qtrly, gdp_hp_qtrly = prepare_country_file(eai_components, real_quarterly_data, cycle_stats, eai_preds, summary_dict, filename)\n",
    "    \n",
    "    log_str += 'Generating additional calculations and saving.\\n'\n",
    "    prepare_sheets(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea41b83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "972d6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. EAI Components Bar Chart\n",
    "def clean_df_bar(fileloc):\n",
    "    df = pd.read_excel(fileloc, sheet_name=2)\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    df = df.dropna(subset=['CONSUMPTION', 'EXO_INTERNATIONAL',\n",
    "       'FINANCIAL', 'GOVERNMENT', 'INVESTMENT', 'TRADE'], how='all') \n",
    "    return df\n",
    "\n",
    "def plotly_bar(df, x_bar, y_line, y_bar_list, plot_fig, write_html=False): \n",
    "    \n",
    "    fig_bar = px.bar(\n",
    "    df, x=x_bar, y=y_bar_list, title=f\"{y_line} components\"\n",
    "    ).add_traces(\n",
    "        px.line(df, x=x_bar, y=y_line).update_traces(showlegend=True, name=y_line).data\n",
    "    ).update_layout(yaxis2={\"side\":\"right\", \"overlaying\":\"y\"}).update_layout(legend = dict(font = dict(family = \"Courier\", size = 15)),\n",
    "                  legend_title = dict(font = dict(family = \"Courier\", size = 15)))\n",
    "    if write_html == True:\n",
    "        #save as html\n",
    "        fig_bar.write_html(f\"{y_line}_components_viz.html\")\n",
    "    if plot_fig == True:\n",
    "        fig_bar.show()\n",
    "    \n",
    "    return fig_bar\n",
    "\n",
    "### 2. Line Chart\n",
    "\n",
    "def target_category(a, b):\n",
    "    if (a == 0.3) and (b == 2.25):\n",
    "        return 'increasing below trend, red'\n",
    "    elif (a == -0.5) and (b == 1.35):\n",
    "        return 'decreasing below trend, yellow'\n",
    "    elif (a == 2.5) and (b == 1.35):\n",
    "        return 'decreasing above trend, blue'\n",
    "    elif (a == 1.7) and (b == 2.25):\n",
    "        return 'increasing above trend, green'\n",
    "    \n",
    "def clean_df_line(fileloc):\n",
    "    df_pred = pd.read_excel(fileloc, sheet_name=3)\n",
    "    df_pred['month'] = pd.to_datetime(df_pred['month'])\n",
    "    \n",
    "    df_tag = pd.read_excel(fileloc, sheet_name=6)\n",
    "    df_tag['target_tag'] = df_tag.apply(lambda x: target_category(x.x_param_eai, x.y_param_eai), \n",
    "                                        axis=1, result_type='expand')\n",
    "    \n",
    "    df_tag = df_tag[['month', 'target_tag']].dropna()\n",
    "    df_tag['month'] = pd.to_datetime(df_tag['month'])\n",
    "    df_tag[['target_tag', 'color']] = df_tag['target_tag'].str.split(',', expand=True)\n",
    "    df_pred = df_pred.merge(df_tag, on='month', how='left')\n",
    "\n",
    "    df_gdp = pd.read_excel(fileloc, sheet_name=5)\n",
    "    df_gdp['qtr'] = pd.to_datetime(df_gdp['qtr'])\n",
    "    df_pred = df_pred.merge(df_gdp[['qtr','GDP_Gap']], left_on='month', right_on='qtr', how='left').drop('qtr', axis=1)\n",
    "    df_pred = df_pred[df_pred.month.dt.year>=2019].reset_index(drop=True)\n",
    "        \n",
    "    df_pred['legend'] = 'EAI'\n",
    "    df_pred['legend2'] = 'Actual GDP Growth Gap'\n",
    "    df_pred['month'] = pd.date_range(start=df_pred.month.iloc[2], freq='M', periods=len(df_pred.month)) #move to end of qtr\n",
    "\n",
    "    return df_pred\n",
    "\n",
    "def clean_df_line_2(df_pred):\n",
    "    df_pred_ci_index = df_pred[(df_pred.EAI.isnull()) & ~(df_pred.PredictedEAI.isnull())].index.tolist()\n",
    "    df_pred_ci_index.append(df_pred_ci_index[0]-1)\n",
    "    df_pred_ci = df_pred.iloc[sorted(df_pred_ci_index)]\n",
    "\n",
    "    df_pred_ci['legend3'] = '95%CiLowerLimit'\n",
    "    df_pred_ci['legend4'] = '95%CiUpperLimit'\n",
    "    df_pred_ci['legend5'] = '99%CiLowerLimit'\n",
    "    df_pred_ci['legend6'] = '99%CiUpperLimit'\n",
    "    df_pred_ci['legend7'] = 'PredictedEAI'\n",
    "    \n",
    "    return df_pred_ci\n",
    "\n",
    "def plot_line(df_pred, df_pred_ci, fileloc, plot_fig): ##updated\n",
    "#     ctry = df_pred['Country'].unique()[0]\n",
    "    fig1 = px.line(df_pred, x=\"month\", y=\"EAI\", color='legend')\n",
    "    fig2 = px.line(df_pred_ci, x=\"month\", y=\"95%CiLowerLimit\", color=\"legend3\", color_discrete_sequence = ['pink'])\n",
    "    fig3 = px.line(df_pred_ci, x=\"month\", y=\"95%CiUpperLimit\", color=\"legend4\", color_discrete_sequence = ['pink'])\n",
    "    fig4 = px.line(df_pred_ci, x=\"month\", y=\"99%CiLowerLimit\", color=\"legend5\", color_discrete_sequence = ['brown'])\n",
    "    fig5 = px.line(df_pred_ci, x=\"month\", y=\"99%CiUpperLimit\", color=\"legend6\", color_discrete_sequence = ['brown'])\n",
    "    fig6 = px.line(df_pred_ci, x=\"month\", y=\"PredictedEAI\", color=\"legend7\", color_discrete_sequence = ['grey'])\n",
    "    fig7 = px.scatter(df_pred, x=\"month\", y=\"GDP_Gap\", color='legend2', color_discrete_sequence=['black'])\n",
    "    fig_line = go.Figure(data = fig1.data + fig2.data + fig3.data + fig4.data + fig5.data + fig6.data + fig7.data)\n",
    "    time = df_pred.month.iloc[df_pred_ci.index[0]]\n",
    "    fig_line.add_vline(x=time, line_color= 'grey', line_dash = 'dash')\n",
    "    fig_line.add_annotation(x=time, y=1, yref=\"paper\", text=\"Predicted Value\")\n",
    "    fig_line.update_layout(title = f\"EAI and the Business Cycle\").update_layout(legend = dict(font = dict(family = \"Courier\", size = 15)),\n",
    "                  legend_title = dict(font = dict(family = \"Courier\", size = 15)))\n",
    "    \n",
    "    if plot_fig == True:\n",
    "        fig_line.show()\n",
    "    \n",
    "    return fig_line\n",
    "\n",
    "### 3. Dial Charts\n",
    "\n",
    "def degree_range(n): \n",
    "    start = np.linspace(0,180,n+1, endpoint=True)[0:-1]\n",
    "    end = np.linspace(0,180,n+1, endpoint=True)[1::]\n",
    "    mid_points = start + ((end-start)/2.)\n",
    "    return np.c_[start, end], mid_points\n",
    "\n",
    "def rot_text(ang): \n",
    "    rotation = np.degrees(np.radians(ang) * np.pi / np.pi - np.radians(90))\n",
    "    return rotation\n",
    "\n",
    "def gauge(ax, labels=['LOW','MEDIUM','HIGH','VERY HIGH','EXTREME'], \\\n",
    "          colors='jet_r', arrow=1, title='', maintitle='', fname=False): \n",
    "    \n",
    "    \"\"\"\n",
    "    some sanity checks first\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(labels)\n",
    "    \n",
    "    if arrow > N: \n",
    "        raise Exception(\"\\n\\nThe category ({}) is greated than \\\n",
    "        the length\\nof the labels ({})\".format(arrow, N))\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    "    if colors is a string, we assume it's a matplotlib colormap\n",
    "    and we discretize in N discrete colors \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(colors, str):\n",
    "        cmap = cm.get_cmap(colors, N)\n",
    "        cmap = cmap(np.arange(N))\n",
    "        colors = cmap[::-1,:].tolist()\n",
    "    if isinstance(colors, list): \n",
    "        if len(colors) == N:\n",
    "            colors = colors[::-1]\n",
    "        else: \n",
    "            raise Exception(\"\\n\\nnumber of colors {} not equal \\\n",
    "            to number of categories{}\\n\".format(len(colors), N))\n",
    "\n",
    "    \"\"\"\n",
    "    begins the plotting\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ang_range, mid_points = degree_range(N)\n",
    "\n",
    "    labels = labels[::-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    plots the sectors and the arcs\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    for ang, c in zip(ang_range, colors): \n",
    "        # sectors\n",
    "        patches.append(Wedge((0.,0.), .4, *ang, facecolor='w', lw=2))\n",
    "        # arcs\n",
    "        patches.append(Wedge((0.,0.), .4, *ang, width=0.10, facecolor=c, lw=2, alpha=0.5))\n",
    "    \n",
    "    [ax.add_patch(p) for p in patches]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    set the labels (e.g. 'LOW','MEDIUM',...)\n",
    "    \"\"\"\n",
    "\n",
    "    for mid, lab in zip(mid_points, labels): \n",
    "\n",
    "        ax.text(0.35 * np.cos(np.radians(mid)), 0.35 * np.sin(np.radians(mid)), lab, \\\n",
    "            horizontalalignment='center', verticalalignment='center', fontsize=10, \\\n",
    "            fontweight='bold', rotation = rot_text(mid))\n",
    "\n",
    "    \"\"\"\n",
    "    set the bottom banner and the title\n",
    "    \"\"\"\n",
    "    r = Rectangle((-0.4,-0.1),0.8,0.1, facecolor='w', lw=2)\n",
    "    ax.add_patch(r)\n",
    "    ax.text(0, -0.08, title, horizontalalignment='center', \\\n",
    "         verticalalignment='center', fontsize=20,\n",
    "           )\n",
    "    \n",
    "    \"\"\"\n",
    "    set the top banner and the title\n",
    "    \"\"\"\n",
    "    r = Rectangle((-0.4,-0.1),0.8,0.1, facecolor='w', lw=2)\n",
    "    ax.add_patch(r)\n",
    "    ax.text(0, 0.45, maintitle, horizontalalignment='center', \\\n",
    "         verticalalignment='center', fontsize=20,\n",
    "           )\n",
    "\n",
    "    \"\"\"\n",
    "    plots the arrow now\n",
    "    \"\"\"\n",
    "    if arrow==0:\n",
    "        pass\n",
    "    else:\n",
    "        pos = mid_points[abs(arrow - N)]\n",
    "\n",
    "        ax.arrow(0, 0, 0.225 * np.cos(np.radians(pos)), 0.225 * np.sin(np.radians(pos)), \\\n",
    "                     width=0.025, head_width=0.06, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "        ax.add_patch(Circle((0, 0), radius=0.02, facecolor='k'))\n",
    "        ax.add_patch(Circle((0, 0), radius=0.01, facecolor='w', zorder=11))\n",
    "\n",
    "    \"\"\"\n",
    "    removes frame and ticks, and makes axis equal and tight\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.set_frame_on(False)\n",
    "    ax.axes.set_xticks([])\n",
    "    ax.axes.set_yticks([])\n",
    "    ax.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        \n",
    "def latest_dial_date(fileloc): \n",
    "    df = pd.read_excel(fileloc, sheet_name=9)\n",
    "    df = df.dropna(subset=['EAI_quarterly']).reset_index(drop=True)\n",
    "    df['quarter'] = pd.to_datetime(df['quarter'])\n",
    "    quarter_number = df.iloc[-1]['quarter'].quarter\n",
    "\n",
    "    df = pd.read_excel(fileloc, sheet_name=6)\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    df = df.dropna(subset=['eai'])\n",
    "    month = df.iloc[-1]['month'].month\n",
    "    year = df.iloc[-1]['month'].year\n",
    "    \n",
    "    return quarter_number, month, year\n",
    "\n",
    "def arrow_category(a, b):\n",
    "    if (a == 0.3) and (b == 2.25):\n",
    "        return 2\n",
    "    elif (a == -0.5) and (b == 1.35):\n",
    "        return 1\n",
    "    elif (a == 2.5) and (b == 1.35):\n",
    "        return 4\n",
    "    elif (a == 1.7) and (b == 2.25):\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def df_clean_quarterly_cycle(fileloc, quarter_number, year):\n",
    "    df = pd.read_excel(fileloc, sheet_name=9)\n",
    "    df = df.dropna(subset=['EAI_quarterly']).reset_index(drop=True)\n",
    "#     ctry = df['Country'].unique()[0]\n",
    "    df['quarter'] = pd.to_datetime(df['quarter'])\n",
    "    \n",
    "    if quarter_number == 1:\n",
    "        quarter_number_month = '01'\n",
    "    elif quarter_number == 2:\n",
    "        quarter_number_month = '04'\n",
    "    elif quarter_number == 3:\n",
    "        quarter_number_month = '07'\n",
    "    elif quarter_number == 4:\n",
    "        quarter_number_month = '10'\n",
    "    df_qtr = df[df.quarter == pd.to_datetime(f'{quarter_number_month}/01/{year}')]\n",
    "    try:\n",
    "        df_qtr['EAI_arrow_cat'] = df_qtr.apply(lambda x: arrow_category(x.x_param_eai_quarterly, x.y_param_eai_quarterly), \n",
    "                                    axis=1, result_type='expand')\n",
    "        eai_values = df_qtr[['prev_eai_quarterly', 'EAI_quarterly', 'EAI_arrow_cat']].values.flatten().tolist()\n",
    "        eai_values.insert(0, 'EAI')\n",
    "    except:\n",
    "        df_qtr['EAI_arrow_cat'] = np.nan\n",
    "        eai_values = ['EAI', np.nan, np.nan, np.nan]\n",
    "    try:\n",
    "        df_qtr['gdp_gap_arrow_cat'] = df_qtr.apply(lambda x: arrow_category(x.x_param_gdp_gap_quarterly, x.y_param_gdp_gap_quarterly), \n",
    "                                    axis=1, result_type='expand')\n",
    "        gdp_gap_values = df_qtr[['prev_gdp_gap_quarterly', 'GDP_gap_quarterly', 'gdp_gap_arrow_cat']].values.flatten().tolist()\n",
    "        gdp_gap_values.insert(0, 'GDP Growth Gap')\n",
    "    except:\n",
    "        df_qtr['gdp_gap_arrow_cat'] = np.nan\n",
    "        gdp_gap_values = ['GDP Growth Gap', np.nan, np.nan, np.nan]\n",
    "    \n",
    "    df_qtr_cycle = pd.DataFrame([eai_values, gdp_gap_values], columns=['sub_title', 'prev', 'curr', 'arrow_cat'])\n",
    "\n",
    "    return df_qtr_cycle \n",
    "\n",
    "def plot_quarterly_cycle(quarter_number, year, fileloc, df_qtr_cycle, plot_fig):\n",
    "\n",
    "    df = pd.read_excel(fileloc, sheet_name=9)\n",
    "    df = df.dropna(subset=['EAI_quarterly']).reset_index(drop=True)\n",
    "#     ctry = df['Country'].unique()[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(21.5, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, axe in enumerate(axes):\n",
    "        df_row = df_qtr_cycle.iloc[i]\n",
    "        maintitle, prev_val, curr_val, arrow_val = df_row[0], df_row[1], df_row[2], df_row[3]\n",
    "        if np.isnan(prev_val):\n",
    "            prev_val = 'None'\n",
    "        else:\n",
    "            prev_val = round(prev_val, 4)\n",
    "        if np.isnan(curr_val):\n",
    "            curr_val = 'None'\n",
    "            arrow_val = 0\n",
    "        else:\n",
    "            curr_val = round(curr_val, 4)\n",
    "            if np.isnan(arrow_val):\n",
    "                arrow_val = 0\n",
    "            else:\n",
    "                arrow_val = int(arrow_val)\n",
    "        \n",
    "        gauge(ax=axe, labels=['BELOW TREND:\\nDECREASING',\n",
    "                  'BELOW TREND:\\nINCREASING',\n",
    "                  'ABOVE TREND:\\nINCREASING',\n",
    "                  'ABOVE TREND:\\nDECREASING'], \\\n",
    "          colors=['#FFCC00','#ED1C24','green','cyan'], \n",
    "          arrow=arrow_val, \n",
    "          title=f'Prev: {prev_val}\\nCurr: {curr_val}', maintitle=maintitle)\n",
    "    fig.suptitle(f'Quarterly Business Cycle Data: Q{quarter_number} {year}', weight='bold', fontsize=20);\n",
    "    if plot_fig == True:\n",
    "        fig.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def arrow_category_components(a):\n",
    "    if a < 0:\n",
    "        return 1 #below target\n",
    "    elif a >= 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def df_sector_dials(fileloc, month, year):\n",
    "    df = pd.read_excel(fileloc, sheet_name=9)\n",
    "    df = df.dropna(subset=['EAI_quarterly']).reset_index(drop=True)\n",
    "#     ctry = df['Country'].unique()[0]\n",
    "\n",
    "    df = pd.read_excel(fileloc, sheet_name=6)\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    df_month = df[df.month == pd.to_datetime(f'{month}/01/{year}')]\n",
    "    df_month['EAI_arrow_cat'] = df_month.apply(lambda x: arrow_category(x.x_param_eai, x.y_param_eai), \n",
    "                                axis=1, result_type='expand')\n",
    "    df_month.apply(lambda x: arrow_category_components(x.con), \n",
    "                                axis=1, result_type='expand')\n",
    "    curr_cols = ['con', 'inv', 'tra', 'fin', 'gov', 'exo']\n",
    "    for col in curr_cols:\n",
    "        df_month[f'{col}_arrow'] = df_month.apply(lambda x: arrow_category_components(x[col]), \n",
    "                                axis=1, result_type='expand')\n",
    "    eai_values = df_month[['eai', 'prev_eai', 'EAI_arrow_cat']].values.flatten().tolist()\n",
    "    eai_values.insert(0, 'All Index')\n",
    "\n",
    "    prev_cols = ['prev_con', 'prev_inv', 'prev_tra', 'prev_fin', 'prev_gov', 'prev_exo']\n",
    "    sector_arrow = ['con_arrow', 'inv_arrow', 'tra_arrow', 'fin_arrow', 'gov_arrow', 'exo_arrow']\n",
    "    sector_full = ['Consumption', 'Investment', 'Trade', 'Finance', 'Government', 'External Sector']\n",
    "    df_curr_val = df_month[curr_cols].reset_index(drop=True).values.flatten().tolist()\n",
    "    df_prev_val = df_month[prev_cols].reset_index(drop=True).values.flatten().tolist()\n",
    "    df_sector_val = df_month[sector_arrow].reset_index(drop=True).values.flatten().tolist()\n",
    "    return eai_values, sector_full, df_prev_val, df_curr_val, df_sector_val \n",
    "\n",
    "def plot_sector_dials(month, year, eai_values, sector_full, df_prev_val, df_curr_val, df_sector_val, plot_fig): \n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(21.5, 10))\n",
    "    fig.delaxes(axes[1,3]) #delete last fig\n",
    "    axes = axes.flatten()\n",
    "    eai_name, curr_eai, prev_eai, eai_arrow = eai_values[0], eai_values[1], eai_values[2], eai_values[3]\n",
    "    if np.isnan(prev_eai):\n",
    "        prev_eai = 'None'\n",
    "    else:\n",
    "        prev_eai = round(prev_eai, 4)\n",
    "    if np.isnan(curr_eai): #if nan, no arrow\n",
    "        curr_eai = 'None'\n",
    "        eai_arrow = 0\n",
    "    else: #if not nan\n",
    "        curr_eai = round(curr_eai, 4)\n",
    "        if np.isnan(eai_arrow):\n",
    "            eai_arrow = 0\n",
    "        else:\n",
    "            eai_arrow = int(eai_arrow)\n",
    "    \n",
    "    gauge(ax=axes[0], labels=['BELOW TREND:\\nDECREASING',\n",
    "                      'BELOW TREND:\\nINCREASING',\n",
    "                      'ABOVE TREND:\\nINCREASING',\n",
    "                      'ABOVE TREND:\\nDECREASING'], \\\n",
    "              colors=['#FFCC00','#ED1C24','green','cyan'], \n",
    "              arrow=eai_arrow, \n",
    "              title=f'Prev: {prev_eai}\\nCurr: {curr_eai}', maintitle=eai_name)\n",
    "    \n",
    "    for i, axe in enumerate(axes[1:-1]):\n",
    "        sector_title = sector_full[i]\n",
    "        prev_val = round(df_prev_val[i], 4)\n",
    "        if np.isnan(prev_val):\n",
    "            prev_val = 'None'\n",
    "        curr_val = round(df_curr_val[i], 4)\n",
    "        if np.isnan(curr_val):\n",
    "                curr_val = 'None'\n",
    "                arrow_val = 0\n",
    "        else:        \n",
    "            arrow_val = int(df_sector_val[i])\n",
    "        gauge(ax=axe, labels=['BELOW TREND',\n",
    "                  'ABOVE TREND'], \\\n",
    "          colors=['#7F00FF','#FF5F15'], \n",
    "          arrow=arrow_val, \n",
    "          title=f'Prev: {prev_val}\\nCurr: {curr_val}', maintitle=sector_title)\n",
    "    fig.suptitle(f'EAI and Sector / Category Dials: {calendar.month_name[month]} {year}', weight='bold', fontsize=15);\n",
    "    \n",
    "    if plot_fig == True:\n",
    "        fig.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "### 4. Save to PDF\n",
    "\n",
    "#plotly\n",
    "def save_pdf_plotly(figures_list, filename, height=900): \n",
    "    \n",
    "    image_list = [pio.to_image(fig, format='png', width=1440, height=height, scale=1.5) for fig in figures_list]\n",
    "    for index, image in enumerate(image_list):\n",
    "        with io.BytesIO() as tmp:\n",
    "            tmp.write(image)  \n",
    "            image = Image.open(tmp).convert('RGB')  # convert and overwrite 'image' to prevent creating a new variable\n",
    "            image_list[index] = image  # overwrite byte image data in list, replace with PIL converted image data\n",
    "\n",
    "    # pop first item from image_list, use that to access .save(). Then refer back to image_list to append the rest\n",
    "    image_list.pop(0).save(filename, 'PDF',\n",
    "                    save_all=True, append_images=image_list, resolution=100.0)  # TODO improve resolution\n",
    "    \n",
    "#plt\n",
    "def save_pdf_plt(figures_list, filename):\n",
    "    with PdfPages(filename) as pdf:\n",
    "        for fig in figures_list:\n",
    "            pdf.savefig(fig) \n",
    "            \n",
    "def merge_pdf(pdf_list, filename):\n",
    "    merger = PdfMerger()\n",
    "\n",
    "    for pdf in pdf_list:\n",
    "        merger.append(pdf)\n",
    "\n",
    "    merger.write(filename)\n",
    "    merger.close()\n",
    "    \n",
    "### 5. Merge All\n",
    "def plot_EAI_dashboard(fileloc, plot_fig=False): \n",
    "    #only 2 bar charts and 1 line chart\n",
    "    df = clean_df_bar(fileloc)\n",
    "    fig_bar_EAI = plotly_bar(df, 'month', \"EAI\", ['CONSUMPTION', 'EXO_INTERNATIONAL',\n",
    "               'FINANCIAL', 'GOVERNMENT', 'INVESTMENT', 'TRADE'], plot_fig)\n",
    "    fig_bar_EAI_GDP = plotly_bar(df, 'month', 'EAI_GDP_gap', \n",
    "            ['CONSUMPTION_GDP_gap', 'EXO_INTERNATIONAL_GDP_gap',\n",
    "           'FINANCIAL_GDP_gap', 'GOVERNMENT_GDP_gap', 'INVESTMENT_GDP_gap',\n",
    "           'TRADE_GDP_gap'], plot_fig)\n",
    "    df_pred = clean_df_line(fileloc)\n",
    "    df_pred_ci = clean_df_line_2(df_pred)\n",
    "    fig_line = plot_line(df_pred, df_pred_ci, fileloc, plot_fig)\n",
    "    save_pdf_plotly([fig_bar_EAI, fig_bar_EAI_GDP, fig_line], f'{fileloc.replace(\"data.xlsx\", \"\")}' + \"EAI_dashboard.pdf\")        \n",
    "\n",
    "    \n",
    "#2. Dial Charts\n",
    "def plot_dial_charts_all(fileloc, plot_fig=False):\n",
    "    quarter_number, month, year = latest_dial_date(fileloc)\n",
    "    fig_list = []\n",
    "    dates_list = pd.date_range(end=f'{month}/01/{year}', periods=36, freq='MS')\n",
    "    quarter_prev = ''\n",
    "\n",
    "    for date in dates_list:\n",
    "        month = date.month\n",
    "        quarter_number = date.quarter\n",
    "        year = date.year\n",
    "        df_qtr_cycle = df_clean_quarterly_cycle(fileloc, quarter_number, year) \n",
    "        eai_values, sector_full, df_prev_val, df_curr_val, df_sector_val = df_sector_dials(fileloc, month, year)\n",
    "        #plot quarter first\n",
    "        if (quarter_prev != quarter_number) and not (df_qtr_cycle.dropna(subset=['prev', 'curr', 'arrow_cat'], how='all').empty): #if new quarter\n",
    "            fig_dial = plot_quarterly_cycle(quarter_number, year, fileloc, df_qtr_cycle, plot_fig)\n",
    "            fig_list.append(fig_dial)\n",
    "            quarter_prev = quarter_number\n",
    "\n",
    "        #plot dials next\n",
    "        if all(np.isnan(df_sector_val)) and (np.isnan(eai_values[-1])): #do not plot if no dial data\n",
    "            pass\n",
    "        else:\n",
    "            fig_dial_sector = plot_sector_dials(month, year, eai_values, sector_full, df_prev_val, df_curr_val, df_sector_val, plot_fig)\n",
    "            fig_list.append(fig_dial_sector)\n",
    "    save_pdf_plt(fig_list, f'{fileloc.replace(\"data.xlsx\", \"\")}' + \"EAI_dial_charts.pdf\")\n",
    "\n",
    "#3. Error\n",
    "def clean_df(fileloc):\n",
    "    df_eai = pd.read_excel(fileloc, sheet_name=4)\n",
    "    df_eai['qtr'] = pd.to_datetime(df_eai['qtr'])\n",
    "\n",
    "    df_gdp = pd.read_excel(fileloc, sheet_name=5)\n",
    "    df_gdp['qtr'] = pd.to_datetime(df_gdp['qtr'])\n",
    "\n",
    "    df = df_eai.merge(df_gdp, on='qtr')[['qtr', 'EAI', 'GDP_Gap']].dropna()\n",
    "    return df\n",
    "\n",
    "def compute_error(df, model_name):\n",
    "    \n",
    "    y_true = df.GDP_Gap.values.tolist()\n",
    "    y_pred = df.EAI.values.tolist()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    df_res = pd.DataFrame([mse, rmse, mae, mape, r2], \n",
    "                          index=['MSE', 'RMSE', 'MAE', 'MAPE', 'R2'], \n",
    "                          columns=[model_name])\n",
    "    return df_res\n",
    "\n",
    "def df_error_metrics_ml(root, save_res=True):\n",
    "    df_res_all = pd.DataFrame()\n",
    "    model_files = model_filepath(root)\n",
    "    for file in model_files:\n",
    "        df = clean_df(file)\n",
    "        df_res = compute_error(df, file) \n",
    "        df_res.columns = [file.replace(\"data.xlsx\",\"\").replace(f\"{root}\",\"\").replace('\\\\', '').replace('/', '')]\n",
    "        df_res_all = pd.concat([df_res_all, df_res], axis=1)\n",
    "    df_res_all = df_res_all.T\n",
    "    if save_res == True:\n",
    "        df_res_all.to_csv(f\"{root}/ml_error_metrics.csv\")\n",
    "    return df_res_all\n",
    "\n",
    "def plot_error_ml(df_error, plot_fig=False):\n",
    "    \n",
    "    fig = make_subplots(rows=2, cols=2,\n",
    "        subplot_titles=[\"RMSE\", \"MAE\", \"R2\"],\n",
    "        horizontal_spacing=0.37)\n",
    "\n",
    "    fig.add_trace(go.Bar(x=df_error.RMSE, y=df_error.index, orientation='h'),\n",
    "                  row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Bar(x=df_error.MAE, y=df_error.index, orientation='h'),\n",
    "                  row=1, col=2)\n",
    "\n",
    "    fig.add_trace(go.Bar(x=df_error.R2, y=df_error.index, orientation='h'),\n",
    "                  row=2, col=1)\n",
    "\n",
    "    fig.update_layout(height=500, width=1000,\n",
    "                      title_text=\"Error Metrics Comparison\", showlegend=False)\n",
    "\n",
    "    fig.update_yaxes(type = 'category', categoryorder='max ascending') \n",
    "    \n",
    "    if plot_fig == True:\n",
    "        fig.show()\n",
    "        \n",
    "    return fig\n",
    "\n",
    "def model_filepath(root):\n",
    "    model_files = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            if 'data.xlsx' in name:\n",
    "                model_files.append(os.path.join(path, name))\n",
    "    return model_files\n",
    "\n",
    "def clean_df_pred_ml(root):\n",
    "    model_files = model_filepath(root)\n",
    "    df_all = clean_df(model_files[0])[['qtr', 'GDP_Gap', 'EAI']]\n",
    "    df_all.rename(columns={\"EAI\":model_files[0].replace(\"data.xlsx\",\"\").replace(f\"{root}\",\"\").replace('\\\\', '').replace('/', '')}, inplace=True) #set to model name\n",
    "    for file in model_files[1:]:\n",
    "        df = clean_df(file).drop('GDP_Gap', axis=1)\n",
    "        df.rename(columns={\"EAI\":file.replace(f\"{root}\",\"\").replace(\"data.xlsx\",\"\").replace('\\\\', '').replace('/', '')}, inplace=True) #set to model name\n",
    "        df_all = df_all.merge(df, on='qtr', how='left')\n",
    "    df_all.qtr = pd.date_range(start=df_all.qtr.iloc[0], freq='Q', periods=len(df_all.qtr)) \n",
    "    return df_all\n",
    "\n",
    "def plot_line_predictions(df, plot_fig=False):\n",
    "    fig_ml = px.line(df, x='qtr', y=df.columns[2:])\n",
    "\n",
    "    fig_gdpgap = go.Figure([go.Scatter(\n",
    "            name='GDP Gap',\n",
    "            x=df['qtr'],\n",
    "            y=df['GDP_Gap'],\n",
    "            line=dict(color=\"black\", width=3.5),\n",
    "            mode='lines+markers',\n",
    "            showlegend=True),\n",
    "            ])\n",
    "    fig3 = go.Figure(data=fig_gdpgap.data + fig_ml.data)\n",
    "    fig3.update_layout(\n",
    "        title='EAI Predictions and GDP Gap',\n",
    "    )\n",
    "    \n",
    "    if plot_fig == True:\n",
    "        fig3.show()\n",
    "        \n",
    "    return fig3\n",
    "\n",
    "def plot_EAI_pred_error(output_path, plot_fig=False):\n",
    "    #line graph\n",
    "    df = clean_df_pred_ml(output_path)\n",
    "    fig_pred = plot_line_predictions(df, plot_fig)\n",
    "    \n",
    "    #error\n",
    "    df_error = df_error_metrics_ml(output_path, save_res=True)  \n",
    "    fig_error = plot_error_ml(df_error, plot_fig)\n",
    "    \n",
    "    #save all\n",
    "    save_pdf_plotly([fig_pred, fig_error], f\"{output_path}/EAI_predictions_comparison.pdf\", 1100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c7ef4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Additional UI Screens and Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fdef77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browse folders for filepath to save all output files\n",
    "#Status update screen (run once process flow is finished)\n",
    "#Status update screen should run all processing, modeling, and final saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "576cc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the progress screen and run all back end functions\n",
    "def UI_progress_screen(event, model_selection, added_data_dict, date_dict, monthly_info, quarterly_info, output_path):\n",
    "    \n",
    "    cancel_button = sg.Button('Cancel', key='CANCEL_OPERATIONS', enable_events = True, font=('Calibri', 8), expand_x=False)\n",
    "    title_txt = sg.Text('Running operations...', font=('Calibri Bold', 14), expand_x=True, justification='center')\n",
    "    instructions_txt = sg.Text('Please wait while EAI predictions are being calculated. If any errors occur, an error message will be displayed. Verify your data inputs carefully. If the issue continues, contact the development team.', font=('Calibri', 10), expand_x=True, justification='center')\n",
    "    status_txt = sg.Multiline(key='TEXTBOX', write_only=True, size=(60,10), font=('Calibri', 8), reroute_cprint=True, expand_x=True, autoscroll=True, auto_refresh=True,)\n",
    "    \n",
    "    model_ls = []\n",
    "    for key in model_selection.keys():\n",
    "        if model_selection[key] == True:\n",
    "            model_ls.append(key)\n",
    "    \n",
    "    progress_ele = sg.ProgressBar(len(model_ls), key='PROGRESS_BAR', size=(60,5), bar_color='white', expand_x=True)\n",
    "    \n",
    "    layout = [\n",
    "        [title_txt],\n",
    "        [instructions_txt],\n",
    "        [status_txt],\n",
    "        [progress_ele],\n",
    "        [cancel_button]\n",
    "        ]\n",
    "    \n",
    "    window = sg.Window('Economic Activity Index: Running Operations', layout, grab_anywhere=True, resizable=True)\n",
    "    event = 'START'\n",
    "    filepath = output_path\n",
    "    \n",
    "    for i in range(len(model_ls)):\n",
    "        event, values = window.read(timeout=0)\n",
    "        if event == 'CANCEL_OPERATIONS' or event == None:\n",
    "            break\n",
    "        else:\n",
    "            exception_msg = ''\n",
    "            try:\n",
    "                model_name = model_ls[i]\n",
    "                model = full_model_dictionary[model_name]\n",
    "\n",
    "                sg.cprint('Preparing data: ' + model_name +' model.', font=('Calibri Bold', 8))\n",
    "                full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict = prep_data(added_data_dict, monthly_info, quarterly_info)\n",
    "                exception_msg = 'Data preparation failed.'\n",
    "\n",
    "\n",
    "                sg.cprint('Generating predictions.', font='Calibri 8')\n",
    "                full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly = generate_ml_predictions(full_df, old_data_monthly, old_data_quarterly, cycles_dict, contributions_dict, date_dict, model)\n",
    "                exception_msg = 'ML prediction generation failed.'\n",
    "\n",
    "                sg.cprint('Calculating initial components.', font='Calibri 8')\n",
    "                eai_comps, rmse_df, ovr_weights, calcs_quarterly, cycle_stats = get_eai_components(full_df, calcs_monthly, calcs_quarterly, stat_dict, data_monthly)\n",
    "                exception_msg = 'Component calculation failed.'\n",
    "\n",
    "                sg.cprint('Calculating data summaries.', font='Calibri 8')\n",
    "                new_rmse_df, summary_dict = calculate_quarterly_summaries(rmse_df)\n",
    "                exception_msg = 'Data summary calculation failed.'\n",
    "\n",
    "                sg.cprint('Calculating overall components.', font='Calibri 8')\n",
    "                eai_components = calculate_overall_components(eai_comps, summary_dict, ovr_weights)\n",
    "                real_quarterly_data = calcs_quarterly[['quarter','HPBasedCycle','CFBasedCycle','HamiltonBasedCycle','GDP_Gap']]\n",
    "\n",
    "                try:\n",
    "                    eai_components['date'] = pd.to_datetime(eai_components['date'])\n",
    "                    eai_components = eai_components.sort_values(by=['date'], ascending = True)\n",
    "                except:\n",
    "                    pass\n",
    "                new_components = eai_components.copy()\n",
    "                comp_ls = list(new_components['EAI'])\n",
    "                count = 0\n",
    "                for i in range(len(new_components)):\n",
    "                    if math.isnan(comp_ls[-i]):\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        break\n",
    "                new_components = new_components[:-(count-1)]\n",
    "                exception_msg = 'Overall component calculations failed.'\n",
    "\n",
    "                sg.cprint('Forecasting the next 3 months.', font='Calibri 8')\n",
    "                eai_preds = predict_EAI(new_components, 3)\n",
    "                real_quarterly_data = real_quarterly_data.set_index('quarter').dropna(axis = 0, how = 'all').reset_index()\n",
    "                exception_msg = 'Forecasting failed.'\n",
    "\n",
    "                sg.cprint('Generating excel data.', font='Calibri 8')\n",
    "                \n",
    "                \n",
    "                Path(filepath+'/'+model_name).mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                filename = filepath +'/'+ model_name + '/'+'data.xlsx'\n",
    "                avg_cycles, eai_comps, eai_preds, eai_qtrly, gdp_hp_qtrly = prepare_country_file(eai_components, real_quarterly_data, cycle_stats, eai_preds, summary_dict, filename)\n",
    "                exception_msg = 'Generating excel data failed..'\n",
    "\n",
    "                sg.cprint('Generating additional calculations and saving.', font='Calibri 8')\n",
    "                prepare_sheets(filename)\n",
    "                exception_msg = 'Additional calculations and file saving failed.'\n",
    "                \n",
    "                #viz per model\n",
    "                sg.cprint('Generating visualizations and saving.', font='Calibri 8')\n",
    "                plot_EAI_dashboard(filename, date_dict)\n",
    "                plot_dial_charts_all(filename)\n",
    "                exception_msg = 'Creating visualizations failed.'\n",
    "            except Exception as e:\n",
    "                error_message = repr(e)\n",
    "                print(error_message)\n",
    "                #sg.cprint(exception_msg +': '+error_message, font='Calibri 8')\n",
    "                sg.cprint('Operations Failed: '+error_message, font='Calibri 8')\n",
    "            window['PROGRESS_BAR'].update_bar(i+1, len(model_ls))\n",
    "    try:\n",
    "        sg.cprint('Generating comparative model analysis.', font=('Calibri Bold', 8))\n",
    "        plot_EAI_pred_error(output_path, date_dict)\n",
    "    except:\n",
    "        sg.cprint('Comparative model analysis generation failed.', font=('Calibri Bold', 8))\n",
    "    sg.popup('Operations completed. Click OK to close the window', font=('Calibri Bold', 12))\n",
    "    window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91d143b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run initial GUI process to gather all relevant information before running back end functions\n",
    "event, model_selection, added_data_dict, date_dict, monthly_info, quarterly_info, output_path = UI_process_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94969d87",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Warning ** Attempting to perform a cprint without a valid window & key Will instead print on Console You can specify window and key in this cprint call, or set ahead of time using cprint_set_output_destination\n",
      "Generating comparative model analysis.\n",
      "** Warning ** Attempting to perform a cprint without a valid window & key Will instead print on Console You can specify window and key in this cprint call, or set ahead of time using cprint_set_output_destination\n",
      "Comparative model analysis generation failed.\n"
     ]
    }
   ],
   "source": [
    "#run back end functions and show progress screen\n",
    "UI_progress_screen(event, model_selection, added_data_dict, date_dict, monthly_info, quarterly_info, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1798aaf",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
